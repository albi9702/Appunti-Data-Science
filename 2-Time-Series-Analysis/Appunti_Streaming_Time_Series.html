<!DOCTYPE html>
<html lang="it" xml:lang="it">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Streaming Data Management and Time Series Analysis</title>
  <meta name="description" content="Streaming Data Management and Time Series Analysis" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Streaming Data Management and Time Series Analysis" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Streaming Data Management and Time Series Analysis" />
  
  
  

<meta name="author" content="Alberto Filosa" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a > Streaming Data Management and Time Series Analysis </a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path=""><a href="#statistical-prediction"><i class="fa fa-check"></i><b>1</b> Statistical Prediction</a><ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#optimal-linear-predictor"><i class="fa fa-check"></i><b>1.1</b> Optimal Linear Predictor</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path=""><a href="#time-series-concepts"><i class="fa fa-check"></i><b>2</b> Time Series Concepts</a><ul>
<li class="chapter" data-level="2.1" data-path=""><a href="#moving-average-process"><i class="fa fa-check"></i><b>2.1</b> Moving Average Process</a></li>
<li class="chapter" data-level="2.2" data-path=""><a href="#autoregressive-process"><i class="fa fa-check"></i><b>2.2</b> Autoregressive Process</a></li>
<li class="chapter" data-level="2.3" data-path=""><a href="#integrated-process"><i class="fa fa-check"></i><b>2.3</b> Integrated Process</a></li>
<li class="chapter" data-level="2.4" data-path=""><a href="#arima-models"><i class="fa fa-check"></i><b>2.4</b> ARIMA Models</a></li>
<li class="chapter" data-level="2.5" data-path=""><a href="#arima-regression"><i class="fa fa-check"></i><b>2.5</b> ARIMA Regression</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path=""><a href="#unobserved-components-model"><i class="fa fa-check"></i><b>3</b> Unobserved Components Model</a><ul>
<li class="chapter" data-level="3.1" data-path=""><a href="#trend"><i class="fa fa-check"></i><b>3.1</b> Trend</a></li>
<li class="chapter" data-level="3.2" data-path=""><a href="#state-space-form"><i class="fa fa-check"></i><b>3.2</b> State Space Form</a><ul>
<li class="chapter" data-level="3.2.1" data-path=""><a href="#kalman-filter"><i class="fa fa-check"></i><b>3.2.1</b> Kalman Filter</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path=""><a href="#stochastic-cycle"><i class="fa fa-check"></i><b>3.3</b> Stochastic Cycle</a></li>
<li class="chapter" data-level="3.4" data-path=""><a href="#seasonality"><i class="fa fa-check"></i><b>3.4</b> Seasonality</a></li>
</ul></li>
<li class="divider"></li>
<li><a href = "https://github.com/rstudio/bookdown" target = "blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Streaming Data Management and Time Series Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Streaming Data Management and Time Series Analysis</h1>
<p class="author"><em>Alberto Filosa</em></p>
<p class="date"><em>28/9/2020</em></p>
</div>
<div style="page-break-after: always;"></div>
<!-- Lecture 1: 28/09/2020 -->
<div id="statistical-prediction" class="section level1">
<h1><span class="header-section-number">1</span> Statistical Prediction</h1>
<p>A <em>Statistical Prediction</em> is a guess about the value of a random variable <span class="math inline">\(Y\)</span> based on the outcome of other random variables <span class="math inline">\(X_1, \dots, X_m\)</span>. A <em>Predictor</em> is a function of the random variables: <span class="math inline">\(\hat Y = p(X_1, \dots, X_m)\)</span>. An optimal prediction is the <em>Loss Function</em> which maps the prediction error to it cost; if the prediction error is zero, also the loss is 0 (exact guess = no losses). A loss function can be symmetric about 0 or asymmetric: <span class="math inline">\(l(-x) = l(x)\)</span>, but the most used is generally asymmetric, like the <em>Quadratic loss function</em>: <span class="math inline">\(l_2(x) = x^2\)</span>.</p>
<p>The predictor <span class="math inline">\(\hat Y = \hat p(X_1, \dots, X_m)\)</span> is optimal for <span class="math inline">\(Y\)</span> with respect to the loss <span class="math inline">\(l\)</span> if minimize the expected loss among the class of measurable functions:</p>
<p><span class="math display">\[\mathbb{E} \ l(Y - \hat Y) = \min_{p \in M}\mathbb{E} \ l[Y - p(X_1, \dots, X_m)]\]</span></p>
<p>The optimal predictor under quadratic loss is the conditional expectation <span class="math inline">\(\hat Y = \mathbb{E}[Y | X_1, \dots, X_m]\)</span>.</p>
<p>The properties of the conditional expectation are:</p>
<ul>
<li><em>Linearity</em>: <span class="math inline">\(\mathbb{E}[aY + bZ + c|X] = a\mathbb{E}[Y|X] + b\mathbb{E}[Z|X] + c\)</span>, with <span class="math inline">\(a,b,c,\)</span> constants;</li>
<li><em>Orthogonality of the prediction error</em>: <span class="math inline">\(\mathbb{E}[(Y - \hat Y) g(x) ] = \mathbb{E}[(Y - \mathbb{E}[Y|X]) g(x) ] = 0\)</span>;</li>
<li><em>Functions of conditioning variables</em>: <span class="math inline">\(\mathbb{E}[Yg(x)|X] = \mathbb{E}[Y|X]g(x)\)</span>;</li>
<li><em>Indipendence with the conditioning variables</em>: <span class="math inline">\(\mathbb{E}[Y|X] = \mathbb{E}[Y]\)</span> if <span class="math inline">\(X \perp\!\!\!\perp Y\)</span>;</li>
<li><em>Law of iterated expecatons</em>: <span class="math inline">\(\mathbb{E}[Y] = \mathbb{E} [\mathbb{E}(Y|X) ]\)</span>;</li>
<li><em>Law of total variance</em>: <span class="math inline">\(\mathbb{V} \text{ar}[Y] = \mathbb{E}[\mathbb{V}\text{ar} (Y|X)] + \mathbb{V} \text{ar} [\mathbb{E}(Y|X)]\)</span>.</li>
</ul>
<div id="optimal-linear-predictor" class="section level2">
<h2><span class="header-section-number">1.1</span> Optimal Linear Predictor</h2>
<p>Sometimes it can be easer to limit the search to a smaller class functions, like linear combination. The main advantage is that the covariance structure of the random variables is all needed to compute the prediction:</p>
<p><span class="math display">\[\mathbb{P}[Y|X_1, \dots, X_m] = \mu_Y + \Sigma_{YX} \Sigma_{XX}^{-1} (X - \mu_X)\]</span></p>
<p>The Properties of the optimal linear predictor are:</p>
<ol style="list-style-type: decimal">
<li><em>Unbiasedness</em>: <span class="math inline">\(\mathbb{E}[Y - \mathbb{P}(Y|X)] = 0\)</span>;</li>
<li><strong><em>M</em></strong>ean <strong><em>S</em></strong>quare <strong><em>E</em></strong>rror (<strong>MSE</strong>) of the prediction: <span class="math inline">\(MSE_{lin} = \Sigma_{YY} - \Sigma_{YX}\Sigma_{YY}^{-1}\Sigma_{XY}\)</span></li>
<li><em>Orthogonality of the prediction error</em>: <span class="math inline">\(\mathbb{E}[(Y - \mathbb{P}[Y|X])X^t] = 0\)</span>;</li>
<li><em>Linearity</em>: <span class="math inline">\(\mathbb{P}[aY + bZ + c| X] = a\mathbb{P}[Y|X] + b\mathbb{P}[Z|X] + c\)</span>;</li>
<li><em>Law of iterated projections</em>: if <span class="math inline">\(\mathbb{E}(X - \mu_x)(Z - \mu_Z)^t = 0\)</span>, then <span class="math inline">\(\mathbb{P}[Y | X,Z] = \mu_Y + \mathbb{P}[Y - \mu_Y|X] + \mathbb{P}[Y - \mu_Y|Z]\)</span>.</li>
</ol>
<!-- Lecture 2: 29/09/2020 -->
</div>
</div>
<div id="time-series-concepts" class="section level1">
<h1><span class="header-section-number">2</span> Time Series Concepts</h1>
<p>A <em>Time Series</em> is a sequence of observation ordered to a time index <span class="math inline">\(t\)</span> taking values in an index set <span class="math inline">\(S\)</span>. If <span class="math inline">\(S\)</span> contains finite numbers we speak about of <em>Discrete</em> time series <span class="math inline">\(y_t\)</span>, otherwise a <em>Continuos</em> time series <span class="math inline">\(y(t)\)</span>.</p>
<p>The most important form of time homogeneity is <em>Stationarity</em>, defined as time-invariance of the whole probability of the data generating process (<em>Strict</em> stationarity) or just of its two moments (<em>Weak</em> stationarity).</p>
<p>The process <span class="math inline">\(\{Y_t\}\)</span> is <em>Strictly</em> stationary if <span class="math inline">\(\forall k \in \mathbb{N}, h \in \mathbb{Z}\)</span> and <span class="math inline">\((t_1, \dots, t_k) \in \mathbb{Z}^k\)</span>,</p>
<p><span class="math display">\[(Y_{t_1}, \dots, Y_{t_k},) \buildrel d \over = (Y_{t_1+h}, \dots, Y_{t_k+ h})\]</span></p>
<p>The process <span class="math inline">\(\{Y_t\}\)</span> is <em>Weakly</em> stationary if, <span class="math inline">\(\forall h, t \in \mathbb{Z}\)</span>, with <span class="math inline">\(\gamma(0) &lt; \infty\)</span></p>
<p><span class="math display">\[\begin{aligned}
    \mathbb{E}(Y_t)                   &amp;= \mu       \\
    \mathbb{C}\text{ov}(Y_t, Y_{t-h}) &amp;= \gamma(h)
\end{aligned}\]</span></p>
<!-- Cioè se media e varianza non dipendono dal tempo e la covarianza del processo con se stesso anticipato, detta funzione di auto-covarianza, dipende solo dalla distanza temporale. -->
<p>If a time series is strictly stationary, then it is also weakly stationary if and only if <span class="math inline">\(\mathbb{V}\text{ar}(Y_t) &lt; \infty\)</span>. If a time series is a Gaussian process, then strict and weak stationarity are equivalent.</p>
<p>The most elementary stationarity process is the White Noise, a time series that show no autocorrelation. A stochastic process is <em>White Noise</em> if has <span class="math inline">\(\mu = 0, \sigma^2 &gt; 0\)</span> and covariance function</p>
<p><span class="math display">\[\gamma(h) =
    \begin{cases}
      \sigma^2 &amp;\quad \text{for } h = 0 \\
      0        &amp;\quad \text{for } h \ne 0
    \end{cases}\]</span></p>
<p><img src="Appunti_Streaming_Time_Series_files/figure-html/White%20Noise-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>If a time series is non-stationary, then the process is called as <em>Random Walk</em>. It is a cumulative sum of a zero mean White Noise series:</p>
<p><img src="Appunti_Streaming_Time_Series_files/figure-html/Random%20Walk-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>The <em>Autocovariance Function</em> is a function characterized by a weakly stationary process, while the <em>Autocorellation Function</em> (<strong>ACF</strong>) is the scale independent version of the autocovariance function. It measures the linear relationship between lagged values of a time series.</p>
<p>If <span class="math inline">\(\{Y_t\}\)</span> is a stationary process with autocovariance <span class="math inline">\(\gamma (·)\)</span>, then its ACF is <span class="math inline">\(p(h) = \mathbb{C}\text{or}(Y_t, Y_{t-h}) = \gamma(h)/\gamma(0)\)</span>.</p>
<p>An alternative summary of the linear dependence of a stationary process can be obtained from the <em>Partial Autocorellation Function</em>, that measures the correlation between <span class="math inline">\(Y_t\)</span> and <span class="math inline">\(Y_{t - h}\)</span> after their linear dependence on the interventing random variables has been removed:</p>
<p><span class="math display">\[\alpha(h) = \mathbb{C}or[Y_t - \mathbb{P}(Y_t | Y_{t-1 | t - h + 1}), 
                           Y_{t - h} - \mathbb{P}(Y_{t - h} | Y_{t-1 | t - h + 1})]\]</span></p>
<!-- Example of ACF and PACF -->
<p>When data have a <em>Trend</em>, autocorrelations for small lags tend to be large and positive because observation nearby in time are also nearby in size, so the ACF tend to have positive values that slowly decrease as the lags increase. When data are seasonal, autocorrelations will be larger for the seasonal lags than for other lags.</p>
<p><img src="Appunti_Streaming_Time_Series_files/figure-html/Autocorrelation%20Plots,-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The slow decrease in ACF as the lags increase is due to the trend, while the scalloped shape is due to seasonality.</p>
<div id="moving-average-process" class="section level2">
<h2><span class="header-section-number">2.1</span> Moving Average Process</h2>
<p>A <strong><em>M</em></strong>oving <strong><em>A</em></strong>verage Process (<strong>MA</strong>) is a process that estimates the trend-cycle at time <span class="math inline">\(t\)</span> obtained by averaging values of the time series within <span class="math inline">\(q\)</span> periods of <span class="math inline">\(t\)</span>. The average eliminates some of the randomness in the data, leaving a smooth trend-cycle component.</p>
<p><span class="math display">\[\begin{aligned}
    Y_t     &amp;= \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q} \\
    Y_{t-1} &amp;= \varepsilon_{t-1} + \theta_1 \varepsilon_{t-2} + \dots + \theta_q \varepsilon_{t-q-1}
  \end{aligned}\]</span></p>
<p>A <span class="math inline">\(MA(q)\)</span> process has <span class="math inline">\(PACF = 0\)</span> for <span class="math inline">\(h &gt; p\)</span> and its characteristic equation <span class="math inline">\(1 + \theta_1X+ \dots + \theta_qX^q = 0\)</span> that has only external solution of the unitary circle is a process such that <span class="math inline">\(Y_t = k + \psi_1 Y_{t-1} + \dots + \psi_q Y_{t-q} + \varepsilon_t\)</span>.</p>
<p>A property of <span class="math inline">\(MA(q)\)</span> models in general is that there are nonzero autocorrelations for the first <span class="math inline">\(q\)</span> lags and autocorrelations = 0 for all lags &gt; <span class="math inline">\(q\)</span>. The theoretical PACF does not shut off, but instead tapers toward 0 in some manner. A clearer pattern for an <span class="math inline">\(MA(q)\)</span> model is in the ACF. The ACF will have non-zero autocorrelations only at lags involved in the model.</p>
<!-- Moving Average Model Identification -->
<p><img src="Immagini/MA(1).png" width="95%" style="display: block; margin: auto;" /></p>
</div>
<div id="autoregressive-process" class="section level2">
<h2><span class="header-section-number">2.2</span> Autoregressive Process</h2>
<p>An Autoregressive Process (<strong>AR</strong>) is a process that forecasts the <span class="math inline">\(Y\)</span> variable using a linear combination of past values of the variable. The term autoregression indicates that it is a regression of the variable against itself.</p>
<p><span class="math display">\[Y_t = k + \phi_1Y_{t-1} + \dots + \phi_pY_{t-p} + \varepsilon_t\]</span></p>
<p>An <span class="math inline">\(AR(p)\)</span> process is stationary for the characteristic equation <span class="math inline">\(1 - \phi_1X - \dots - \phi_pX^p = 0\)</span> and all solutions are external of the unitary circle</p>
<p>If <span class="math inline">\(Y_t\)</span> is stationary, <span class="math inline">\(\mathbb{E}(Y_t) = \frac{k}{1- \phi_1 - \dots - \phi_p}\)</span>.</p>
<p>The <span class="math inline">\(AR(1)\)</span> process is <span class="math inline">\(Y_t = k + \phi Y_{t-1} + \varepsilon_1\)</span>, so <span class="math inline">\(1 - \phi x = 0 \implies x = 1/\phi\)</span>. <span class="math inline">\(AR(1)\)</span> is stationary if <span class="math inline">\(|\phi | &lt; 1\)</span>. If in <span class="math inline">\(AR(1)\)</span> <span class="math inline">\(\phi = 1\)</span> we obtain a non stationary process (integrate) called Random Walk:</p>
<p><span class="math display">\[Y_t = k + Y_{t-1} + \varepsilon\]</span></p>
<p>For a positive value of <span class="math inline">\(\phi_p\)</span>, the ACF exponentially decreases to 0 as the lag increases. For a negative value, the ACF also exponentially decays to 0 as the lag increases, but the algebric signs for the autocorrelations alternate between positive and negative. The theoretical PACF <em>shuts off</em> (close to 0) past the order of the model.</p>
<!-- Autoregressive Model Identification -->
<p><img src="Immagini/AR(1).png" width="95%" style="display: block; margin: auto;" /></p>
<!-- Lecture 3: 01/10/2020 -->
</div>
<div id="integrated-process" class="section level2">
<h2><span class="header-section-number">2.3</span> Integrated Process</h2>
<p>An <em>Integrated Process</em> <span class="math inline">\(\{ Y_t \} \sim I(d)\)</span> is a non stationary process, but its first difference is stationary:</p>
<p><span class="math display">\[\Delta Z_t = Z_t - Z_{t-1} \sim I(0)\]</span></p>
<p>A <span class="math inline">\(Z_t\)</span> process is integrated of order <span class="math inline">\(d\)</span> if it is not stationary, <span class="math inline">\(\Delta^{d-1}Z_{t-1}\)</span> non stationary, while <span class="math inline">\(\Delta^dZ_t\)</span> is stationary.</p>
<p>The process <span class="math inline">\(\{Y_t\}\)</span> is ARMA<span class="math inline">\((p,q)\)</span> if it is stationary and satisfies:</p>
<p><span class="math display">\[Y_t = \phi_1Y_{t-1} + \dots + \phi_pY_{t-p} + \theta_1Z_{t-1} + \dots + \theta_1Y_{t-q}\]</span></p>
<p>An <span class="math inline">\(AR(p)\)</span> process is integrated in order <span class="math inline">\(d\)</span> if there are <span class="math inline">\(d\)</span> solution <span class="math inline">\(x = 1\)</span> and the other <span class="math inline">\(|x| &gt; 1|\)</span>.</p>
</div>
<div id="arima-models" class="section level2">
<h2><span class="header-section-number">2.4</span> ARIMA Models</h2>
<p>If we combine differencing with autoregression and a moving average model, we obtain a non-seasonal <strong><em>A</em></strong>uto<strong><em>R</em></strong>egressive <strong><em>I</em></strong>ntegrated <strong><em>M</em></strong>oving <strong><em>A</em></strong>verage (<strong>ARIMA</strong>).</p>
<p><span class="math display">\[Y_t&#39; = c + \phi_1 Y_{t-1}&#39; + \dots + \phi_p Y_{t-p}&#39; + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q} + \varepsilon_t\]</span></p>
<p>where <span class="math inline">\(Y_t&#39;\)</span> is the differenced series. This is an <span class="math inline">\(ARIMA(p, d, q)\)</span>, where <span class="math inline">\(p\)</span> is the order of the <strong><em>A</em></strong>uto<strong><em>R</em></strong>egressive part, <span class="math inline">\(d\)</span> is the degree of first differencing involved (<strong><em>I</em></strong>ntegrated) and <span class="math inline">\(q\)</span> is the order of the <strong><em>M</em></strong>oving <strong><em>A</em></strong>verage part.</p>
<p>The backward shift operator <span class="math inline">\(B(y_t) = y_{t-1}\)</span> is a useful notational device when working with time series lags, so it has the effect of shifting the data back one period. The backward shift operator is convenient for describing the process of differencing. A first difference can be written as:</p>
<p><span class="math display">\[y_t&#39; = y_t - y_{t-1} = y_t - B(y_t) = (1 - B)y_t\]</span></p>
<p>Backshift notation is particularly useful when combining differences, as the operator can be treated using ordinary algebraic rules. In particular, terms involving <span class="math inline">\(B\)</span> can be multiplied together.</p>
<!-- Box and Jenkins Procedure for ARIMA Model Identification -->
<p><img src="Immagini/Box-Jenkins-Procedure.png" width="95%" style="display: block; margin: auto;" /></p>
<!-- Lecture 4 04/10/2020 -->
</div>
<div id="arima-regression" class="section level2">
<h2><span class="header-section-number">2.5</span> ARIMA Regression</h2>
<p>The time series models allow for the inclusion of information from past observations of a series, but not for the inclusion of other information that may also be relevant:</p>
<p><span class="math display">\[\begin{aligned}
    \Delta^d y_t                        &amp;= \beta_t \Delta^d X_t + \frac{\theta_q(B)}{\phi_p(B)} \varepsilon_t \\
    \Delta^d(y_t - \beta_t X_t)         &amp;= \frac{\theta_q(B)}{\phi_p(B)} \varepsilon_t                        \\
    \phi(B) \Delta^d(y_t - \beta_t X_t) &amp;= \theta(B) \varepsilon_t
  \end{aligned}\]</span></p>
<p>The data may follow an <span class="math inline">\(ARIMA(p, d, 0)\)</span> model if the ACF is exponentially decaying or sinusoidal and there is a significant spike at lag <span class="math inline">\(p\)</span> in PACF but none beyond lag <span class="math inline">\(p\)</span>.</p>
<p>The data may follow an <span class="math inline">\(ARIMA(0, d, q)\)</span> model if the there is a significant spike at lag <span class="math inline">\(q\)</span> in ACF but none beyond lag <span class="math inline">\(q\)</span> and PACF is exponentially decaying or sinusoidal.</p>
<!-- https://online.stat.psu.edu/stat510/lesson/1 -->
<!-- Lecture 5 06/10/2020 -->
</div>
</div>
<div id="unobserved-components-model" class="section level1">
<h1><span class="header-section-number">3</span> Unobserved Components Model</h1>
<p>A natural way humans tend to think about time series is as sum of non directly observable components such as trends, seasonality and a component cycle. <strong><em>U</em></strong>nobserved <strong><em>C</em></strong>omoponents <strong><em>M</em></strong>odels (<strong>UCM</strong>) select the best features of stochastic framework as ARIMA models, but also they tend to perform very well in forecasting.</p>
<p>The observable time series of UCM is the sum of Trend (<span class="math inline">\(\mu_t\)</span>), Cycle (<span class="math inline">\(\psi_t\)</span>), Seasonality (<span class="math inline">\(\gamma_t\)</span>) and White Noise (<span class="math inline">\(\varepsilon_t\)</span>):</p>
<p><span class="math display">\[Y_t = \mu_t + \psi_t + \gamma_t + \varepsilon_t\]</span></p>
<p>Some of these components could be skipped and some other could be added. Also, they can be seen as stochastic version of the deterministic functions of time. In the next sections we see how to build stochastically evolving components.</p>
<div id="trend" class="section level2">
<h2><span class="header-section-number">3.1</span> Trend</h2>
<p>The <em>Trend</em> is responsible for the mean variation process, usually adopted in UCM as the <strong><em>L</em></strong>ocal <strong><em>L</em></strong>inear <strong><em>T</em></strong>rend (<strong>LLT</strong>). Let us take a linear function defined as follows:</p>
<p><span class="math display">\[\mu_t = \mu_0 + \beta_0 t\]</span></p>
<p>where <span class="math inline">\(\mu_0\)</span> is the intercept and <span class="math inline">\(\beta_0\)</span> the slope and write it in incremental form:</p>
<p><span class="math display">\[\mu_t = \mu_{t-1} + \beta_0\]</span></p>
<p>By adding the White Noise <span class="math inline">\(\eta_n\)</span>, we obtain a Random Walk with Drift. In this case we can interpreted <span class="math inline">\(\mu_t\)</span> as a linear trend with a Random Walk intercept, but the slope remains unchanged.</p>
<p>It is possible obtain a time-varying slope in trends, easily achieved letting the slope evolves as a Random Walk:</p>
<p><span class="math display">\[\begin{aligned}
    \mu_t   &amp;= \mu_{t-1}   + \beta_0 + \eta_n \\
    \beta_t &amp;= \beta_{t-1} + \xi_t            
\end{aligned}\]</span></p>
<p>These equations define the local linear trend interpreted as a linear trend where both intercept and slope evolve in time as Random Walks.</p>
<p>The local linear trend has different special case of interest obtained by fixing the value of the variances:</p>
<ul>
<li><em>Deterministic Linear Trend</em> if <span class="math inline">\(\sigma_{\eta}^2 = \sigma_{\xi}^2 = 0\)</span>;</li>
<li><em>Random Walk with Drift</em> <span class="math inline">\(\beta_0\)</span> if <span class="math inline">\(\sigma_{\xi}^2 = 0\)</span> (slope constant);</li>
<li><em>Random Walk</em> if <span class="math inline">\(\sigma_{\xi}^2 = \beta_0 = 0\)</span> (slope = 0);</li>
<li><em>Integrated Random Walk</em> if <span class="math inline">\(\sigma_{\eta}^2 = 0\)</span>, with a very smooth trend.</li>
</ul>
<p><strong>Obs.</strong>: the local linear trend can be also seen as an ARIMA process: if <span class="math inline">\(\sigma_{\xi}^2 &gt; 0\)</span>, <span class="math inline">\(\mu_t \sim I(2)\)</span> process as trend is non-stationary, while its second difference is stationary.</p>
<!-- Lecture 6: 08/10/2020 -->
</div>
<div id="state-space-form" class="section level2">
<h2><span class="header-section-number">3.2</span> State Space Form</h2>
<p>The state space form is a system of equation in which one or more observable time series are linearly related to a set of unobservable state variables. It is defined by the following system of equation:</p>
<p><span class="math display">\[\begin{cases}
      Y_t          &amp;= d_t + Z_t \alpha_t + \varepsilon_t
      \quad \text{Observation Equation} \\
      \alpha_{t+1} &amp;= c_t + T_t \alpha_t + R_t \eta_t
      \quad \text{State Equation}
  \end{cases}\]</span></p>
<p>where <span class="math inline">\(\varepsilon_t \sim WN(0, H_t)\)</span>, <span class="math inline">\(\eta_t \sim WN(0, Q_t)\)</span>, uncorrelated random variables with zero mean and covariance matrix respectably <span class="math inline">\(H_t\)</span> and <span class="math inline">\(Q_t\)</span>.</p>
<p>The initialization is the process where the state vector <span class="math inline">\(a_{1|0} = \mathbb{E}(\alpha_1)\)</span> and <span class="math inline">\(P_{1|0} = \mathbb{E}(\alpha_1 - a_{1|0})(\alpha_1 - a_{1|0})^t\)</span>.</p>
<p><strong>Example.</strong> <span class="math inline">\(\qquad\)</span> Let’s transform a Local Linear Trend with White Noise in the State Space form. The LLT has this system of equation:</p>
<p><span class="math display">\[\begin{aligned}
    \mu_t   &amp;= \mu_{t-1}   + \beta_0 + \eta_n \\
    \beta_t &amp;= \beta_{t-1} + \xi_t            
\end{aligned}\]</span></p>
<p>with noise <span class="math inline">\(y_t = \mu_t + \varepsilon_t\)</span>. The state space form of the LLT is the following:</p>
<p><span class="math display">\[\begin{cases}
      \alpha_{t+1} = T \alpha_t + R \eta_t = 
                 \begin{bmatrix}
                   \mu_{t+1} \\
                   \beta_{t+1}
                 \end{bmatrix} =
                 \begin{bmatrix}
                   1 &amp; 1 \\
                   0 &amp; 1
                 \end{bmatrix}
                 \begin{bmatrix}
                   \mu_t \\
                   \beta_t
                   \end{bmatrix} + 
                 \begin{bmatrix}
                   \eta_t \\
                   \zeta_t
                 \end{bmatrix}
      \quad \text{State Equation} \\
      Y_t = Z \alpha_t + \varepsilon_t =
             \begin{bmatrix}
               1 &amp; 0
             \end{bmatrix}
             \begin{bmatrix}
               \mu_t \\
               \beta_t
             \end{bmatrix} + \varepsilon_t
      \quad \text{Observation Equation}
  \end{cases}\]</span></p>
<p>where <span class="math inline">\(\eta_t = \begin{bmatrix} \eta_t \\ \zeta^2 \end{bmatrix}\)</span> and <span class="math inline">\(Q = \begin{bmatrix} \sigma_\eta^2 &amp; 0 \\ 0 &amp; \sigma_\zeta^2 \end{bmatrix}\)</span>.</p>
<p>The initialization process is the following:</p>
<p><span class="math display">\[a_{1|0} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} 
  \qquad
  P_{1|0} = \begin{bmatrix} \infty &amp; 0 \\ 0 &amp; \infty \end{bmatrix}\]</span></p>
<p><strong>Obs.</strong> <span class="math inline">\(\qquad\)</span> The Integrated Random Walk is a LLT with the variance of the first disturbance set to zero.</p>
<!-- Lecture 7: 13/10/2020-->
<div id="kalman-filter" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Kalman Filter</h3>
<p>Suppose that all parameters in the state space form are known; the only unknown quantities are unobservable components, specified as random variables, then the inference to carry out is the statistical prediction:</p>
<ul>
<li>Forecasting, the SP of <span class="math inline">\(\alpha_t\)</span> based on <span class="math inline">\(Y_s\)</span> (<span class="math inline">\(s &lt; t\)</span>);</li>
<li>Filtering, SP of <span class="math inline">\(\alpha_t\)</span> based on <span class="math inline">\(Y_t\)</span>;</li>
<li>Smoothing, SP of <span class="math inline">\(\alpha_t\)</span> based on <span class="math inline">\(Y_s\)</span> (<span class="math inline">\(s &gt; t\)</span>).</li>
</ul>
<p>It is possible to build the optimal linear predictor that is better than any other predictor assuming if <span class="math inline">\(\varepsilon_t\)</span>, <span class="math inline">\(v_t \sim WN\)</span> and the initial state <span class="math inline">\(\alpha_1\)</span> are jointly Gaussian.</p>
<p>Consider the following notation:</p>
<p><span class="math display">\[\begin{aligned}
    a_{t|s} &amp;= \mathbb{P}[\alpha_t | Y_t] \\
    P_{t|s} &amp;= \mathbb{E}[\alpha_t - a_{t|s}][\alpha_t - a_{t|s}]^t
  \end{aligned}\]</span></p>
<p>The <em>Kalman Filter</em> is an algorithm for computing the pair <span class="math inline">\(\{a_{t|t-1}, P_{t|t-1}\}\)</span> starting from <span class="math inline">\(\{a_{t-1|t-1}, P_{t-1|t-1}\}\)</span> and <span class="math inline">\(\{a_{t|t}, P_{t|t}\}\)</span> starting from <span class="math inline">\(\{a_{t|t-1}, P_{t|t-1}\}\)</span>. Perciò proietta le stime di <span class="math inline">\(y_t\)</span> partendo basate sulle osservazioni precedenti.</p>
<p>It also provides the sequence of innovations with the relative covariance matrix, used to compute the Gaussian likelihood of the model in state space form.</p>
<ul>
<li>Prediction Step:</li>
</ul>
<p><span class="math display">\[\begin{cases}
    a_{t|t-1} &amp;= \mathbb{P}(\alpha_t | y_1, \dots, y_{t-1}) \\
    P_{t|t-1} &amp;= \mathbb{E}[\alpha_t - a_{t | t-1}][\alpha_t - a_{t | t-1}]^t
  \end{cases}\]</span></p>
<ul>
<li>One-Step-Ahead Forecast and Innovation:</li>
</ul>
<p><span class="math display">\[\begin{cases}
    \hat y_{t | t-1} &amp;= \mathbb{P}(\alpha_t | y_1, \dots, y_{t-1}) \\
    i_t              &amp;= y_t - \hat y_{t | t-1} \\
    F_t              &amp;= \mathbb{E}[i_t i_t^t] = \mathbb{E}[y_t - \hat y_{t | t-1}][y_t - \hat y_{t | t-1}]^t
  \end{cases}\]</span></p>
<ul>
<li>Updating Step:</li>
</ul>
<p><span class="math display">\[\begin{cases}
    a_{t | t} &amp;= \mathbb{P}(\alpha_t | y_1, \dots, y_t) \\
    P_{t|t}  &amp;= \mathbb{E}[\alpha_t - a_{t | t}][\alpha_t - a_{t | t}]^t
  \end{cases}\]</span></p>
<p><span class="math display">\[(a_{1|0}, P_{1|0}) \rightarrow (a_{1|1}, P_{1|1}) \rightarrow (a_{2|1}, P_{2|1}) \rightarrow \ldots (a_{n|n}, P_{n|n})\]</span></p>
<p>The linear prediction of the state vector <span class="math inline">\(\alpha_t\)</span> based on <span class="math inline">\(y_s = \{Y_1, \dots, Y_s\}\)</span> is called <em>Smoothing</em>. The fixed interval smoother provides prediction for the state vector based on the whole time series:</p>
<p><span class="math display">\[a_{t | n} = \mathbb{P}[\alpha_t | Y_n]\]</span></p>
<p><span class="math display">\[\begin{aligned} 
    \hat y_{t | t - 1} &amp;= \mathbb{P}(y_t | y_{t - 1}) \\
    \hat y_{t | n}     &amp;= \mathbb{P}(y_t | y_n)
  \end{aligned}\]</span></p>
<p>The Disturbance Smoothing computes the prediction of the White Noise sequences based on the whole time series, useful for identifying outliers:</p>
<p><span class="math display">\[\begin{aligned}
    \hat \varepsilon_{t | n} &amp;= \mathbb{P}(\varepsilon_t | y_n) \\
    V_{t | n}^{\varepsilon} &amp;= \mathbb{E}[\varepsilon_t - \hat \varepsilon_{t | n}] [\varepsilon_t - \hat \varepsilon_{t | n}] ^ t \\
    U_{t | n}^{\varepsilon} &amp;= \mathbb{E}[\hat \varepsilon_{t | n}] \hat \varepsilon_{t | n}] ^ t] \\
    \hat \eta_{t | n} &amp;= \mathbb{P}(\eta_n | y_n) \\
    V_{t | n}^{\varepsilon} &amp;= \mathbb{E}[(\eta_n - \hat \eta_{t | n}) (\eta_n - \hat \eta_{t | n}) ^ t] \\
    U_{t | n}^{\varepsilon} &amp;= \mathbb{E}[\hat \eta_{t | n} \hat \eta_{t | n}] ^ t]
  \end{aligned}\]</span></p>
<p>If observation are Normally Distributed, the Kalman Filter allows the construction of the likelihood function. Using the definition of conditional density, it is possible to factorise the joint density of the data:</p>
<p><span class="math display">\[f_{\theta}(y_1, \dots, y_n) = f_{\theta}(y_1) f_{\theta}(y_2 | y_1) \dots f_{\theta}(y_n | y_{n -1}, \dots, y_1)\]</span></p>
<p>Under this assumption, the conditional distribution is also Normal and the Kalman Filter provides the conditional mean and variance:</p>
<p><span class="math display">\[\mathbb{E}(Y_t | Y_1, dots, Y_{t -1}) = \hat y_{t | t - 1}
\mathbb{V}ar(Y_t | Y_1, dots, Y_{t -1}) = F_t\]</span></p>
<p>The likelihood is the joint density of the sample path of the unknown parameters <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[L(\theta) = \prod_{t = 1}^n f_{\theta}(y_t | y_{t - 1}, \dots, y_1)\]</span></p>
<p>It is generally better to work on the log-likelihood function:</p>
<p><span class="math display">\[\begin{aligned}
    l(\theta) &amp;= \log f(y_1, \dots, y_n) = \sum_{t=1}^n \log f(y_t | y_1, \dots, y_{t-1}) = \\
              &amp;= \sum_{t=1}^n - 1/2(\log |F_t(\theta)| +                                    \\
              &amp;= (y_t - \hat y_{t | t-1}(\theta)^t F_t(\theta)^{-1} (y_t - \hat y_{t | t-1}(\theta)^t \\
              &amp; \implies \hat \theta_n = \max l(\theta)
  \end{aligned}\]</span></p>
<!-- Dummy variables -->
<!-- Lecture 8: 19/10/2020 -->
</div>
</div>
<div id="stochastic-cycle" class="section level2">
<h2><span class="header-section-number">3.3</span> Stochastic Cycle</h2>
<p>A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency. The natural deterministic function for generating a <em>Cycle</em> with frequency <span class="math inline">\(\lambda\)</span> is the sinusoid <span class="math inline">\(R \cos(\lambda t + \phi)\)</span> where <span class="math inline">\(R\)</span> is the amplitude and <span class="math inline">\(\phi\)</span> the phase. A geometrical way to generate a sinusoid is taking a circumference of radius equal to <span class="math inline">\(R\)</span> and letting a point move on it. The <span class="math inline">\(\psi_t\)</span> cycle is stationary:</p>
<p><span class="math display">\[\begin{bmatrix}
    \psi_{t+1} \\
    \psi_{t+1}^*
  \end{bmatrix}
  =
  \rho
  \begin{bmatrix}
    \cos(\lambda)  &amp; \sin(\lambda) \\
    -\sin(\lambda) &amp; \cos(\lambda)
  \end{bmatrix}
  \begin{bmatrix}
    \psi_{t} \\
    \psi_{t}^*
  \end{bmatrix}
  +
  \begin{bmatrix}
    k_t \\
    k_t^*
  \end{bmatrix}\]</span></p>
<p>where <span class="math inline">\(\rho \in [0, 1]\)</span> is the damping factor, <span class="math inline">\(\lambda \in [0, \pi]\)</span> the frequency of the cycle and <span class="math inline">\(k_t\)</span>, <span class="math inline">\(k_t^*\)</span> are indipendent WN sequences with common variance <span class="math inline">\(\sigma_k^2\)</span>.</p>
<!-- Geometrical Sinusoid Image -->
<p><img src="Immagini/Geometric-Sinusoid.PNG" width="95%" style="display: block; margin: auto;" /></p>
<p>There is a unique casual and stationary solution with <span class="math inline">\(\mathbb{E}[\psi_t] = 0\)</span>, <span class="math inline">\(\mathbb{E}[\psi_{t+h} \psi_t^t] = \frac{\sigma_k^2}{1 - \rho^2}I^2\)</span> and <span class="math inline">\(\psi_t \sim ARMA(2, 1)\)</span> process with complex roots in the AR polynomial.</p>
<p>When <span class="math inline">\(\rho = 1\)</span> and <span class="math inline">\(\mathbb{E}[\psi_t] = 0\)</span>, the cycle is not stationary, while if <span class="math inline">\(\rho &lt; 1\)</span> the cycle is stationary.</p>
</div>
<div id="seasonality" class="section level2">
<h2><span class="header-section-number">3.4</span> Seasonality</h2>
<p>A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality always regards a fixed and known frequency. The seasonal component is generally modelled using the stochastic dummy form and the trigonometric one.</p>
<p>Consider a time series with seasonality <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[\sum_{j = 1}^{s/2} a_j \cos(\frac{2 \pi}{s} jt) + b_j \sin(\frac{2 \pi}{s} jt)\]</span></p>
<p>The <em>Trigonometric Form</em> is given by <span class="math inline">\(\gamma_t = \sum_{j=1}^{s/2} \gamma_t^{(j)}\)</span>, <span class="math inline">\(\gamma_t^{(j)}\)</span> is the nonstationarity stochastic cycle:</p>
<p><span class="math display">\[\begin{bmatrix}
    \gamma_{t+1}^{(j)} \\
    \gamma_{t+1}^{*(j)}
  \end{bmatrix}
  =
  \begin{bmatrix}
    \cos(\frac{2j \pi}{s})  &amp; \sin(\frac{2j \pi}{s}) \\
    -\sin(\frac{2j \pi}{s}) &amp; \cos(\frac{2j \pi}{s})
  \end{bmatrix}
  \begin{bmatrix}
    \gamma_{t}^{(j)} \\
    \gamma_{t}^{*(j)}
  \end{bmatrix}
  +
  \begin{bmatrix}
    w_t^{(j)} \\
    w_t^{*(j)}
  \end{bmatrix}\]</span></p>
<p>The arguments of the sinusoids are called seasonal frequencies. When <span class="math inline">\(s\)</span> is even <!--pari--> and <span class="math inline">\(j = s/2\)</span>, the second equation of the previous system can be omitted because <span class="math inline">\(\sin(\pi) = 0\)</span> and the first equation is reduced as <span class="math inline">\(\gamma_{t+1}^{(s/2)} = - \gamma_{t}^{(s/2)} - w_{t}^{(s/2)}\)</span> which does not depend on the value of the second element.</p>
<p>An alternative way (dummy form) of modeling seasonal component is defining <span class="math inline">\(s\)</span> variables that evolve as RW. Let <span class="math inline">\(\gamma_t\)</span> represent the effect of the season a time <span class="math inline">\(t\)</span>,</p>
<p><span class="math display">\[\gamma_t = - \sum_{s = 1}^{s - 1}\gamma_{t-i} + w_t\]</span></p>
<p>where <span class="math inline">\(w_t \sim WN(0, \sigma^2_w)\)</span>. The most straightforward way to make this component evolve stochastically over time is by adding mean-zero random shocks.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
