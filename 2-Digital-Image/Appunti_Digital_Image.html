<!DOCTYPE html>
<html lang="it" xml:lang="it">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Digital Signal and Image Management</title>
  <meta name="description" content="Digital Signal and Image Management" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Digital Signal and Image Management" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Digital Signal and Image Management" />
  
  
  

<meta name="author" content="Alberto Filosa" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a > Digital Signal and Image Management </a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path=""><a href="#classificazione-dei-segnali"><i class="fa fa-check"></i><b>1</b> Classificazione dei Segnali</a></li>
<li class="chapter" data-level="2" data-path=""><a href="#analisi-di-fourier"><i class="fa fa-check"></i><b>2</b> Analisi di Fourier</a><ul>
<li class="chapter" data-level="2.1" data-path=""><a href="#trasformata"><i class="fa fa-check"></i><b>2.1</b> Trasformata</a></li>
<li class="chapter" data-level="2.2" data-path=""><a href="#convoluzioni"><i class="fa fa-check"></i><b>2.2</b> Convoluzioni</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path=""><a href="#fondamenti-di-immagini-digitali"><i class="fa fa-check"></i><b>3</b> Fondamenti di Immagini Digitali</a><ul>
<li class="chapter" data-level="3.1" data-path=""><a href="#miglioramento-immagini"><i class="fa fa-check"></i><b>3.1</b> Miglioramento Immagini</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path=""><a href="#color-spaces"><i class="fa fa-check"></i><b>4</b> Color Spaces</a></li>
<li class="chapter" data-level="5" data-path=""><a href="#interest-point-detectors-and-descriptors"><i class="fa fa-check"></i><b>5</b> Interest Point Detectors and Descriptors</a><ul>
<li class="chapter" data-level="5.1" data-path=""><a href="#local-descriptors-trends"><i class="fa fa-check"></i><b>5.1</b> Local Descriptors Trends</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path=""><a href="#trainable-classifiers"><i class="fa fa-check"></i><b>6</b> Trainable Classifiers</a><ul>
<li class="chapter" data-level="6.1" data-path=""><a href="#support-vector-machine"><i class="fa fa-check"></i><b>6.1</b> Support Vector Machine</a></li>
<li class="chapter" data-level="6.2" data-path=""><a href="#neural-network"><i class="fa fa-check"></i><b>6.2</b> Neural Network</a></li>
<li class="chapter" data-level="6.3" data-path=""><a href="#convolutional-neural-network"><i class="fa fa-check"></i><b>6.3</b> Convolutional Neural Network</a></li>
<li class="chapter" data-level="6.4" data-path=""><a href="#transfer-learning"><i class="fa fa-check"></i><b>6.4</b> Transfer Learning</a></li>
</ul></li>
<li class="divider"></li>
<li><a href = "https://github.com/rstudio/bookdown" target = "blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Digital Signal and Image Management</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Digital Signal and Image Management</h1>
<p class="author"><em>Alberto Filosa</em></p>
<p class="date"><em>29/9/2020</em></p>
</div>
<div id="classificazione-dei-segnali" class="section level1">
<h1><span class="header-section-number">1</span> Classificazione dei Segnali</h1>
<p>I segnali possono essere classificati in base al Dominio ed al Codominio. Si riportano i seguenti esempi:</p>
<ol style="list-style-type: decimal">
<li>Segnale Analogico, <span class="math inline">\(\mathbb{D} : \mathbb{R} \rightarrow \mathbb{R}\)</span>;</li>
<li>Segnale Analogico a tempo discreto, <span class="math inline">\(\mathbb{D} : \mathbb{R} \rightarrow \mathbb{K}\)</span>, con <span class="math inline">\(K = \{\dots, t-1, t, t+1, \dots\}\)</span>;</li>
<li>Segnale Continuo nelle ampiezze, <span class="math inline">\(\mathbb{D} : \mathbb{R} \rightarrow \mathbb{K}\)</span>;</li>
<li>Segnale Digitale, <span class="math inline">\(\mathbb{D} : \mathbb{K} \rightarrow \mathbb{K}\)</span>.</li>
</ol>
<p>Per rappresentare digitalmente un segnale analogico sono necessari 3 fasi:</p>
<ul>
<li>Campionamento: si considerano solamente le parti di segnali tali per cui non si perdono troppe informazioni. Una alta frequenza di campionamento significa una buona riproduzione del segnale originale, ma si avrà un numero elevato di dati, mentre una bassa frequenza di campionamento produce il fenomeno chiamato aliasing;</li>
<li>Quantizzazione: l’ADC campiona una onda analogica ad intervalli temporali uniformi ed assegna u valore digitale ad ogni campione. Il valore è ottenuto tramite la seguente formula:</li>
</ul>
<p><span class="math display">\[\text{Digital Output Code} = \frac{\text{Analog Input}}{\text{Reference Input}} \times (2^N - 1)\]</span>;</p>
<ul>
<li>Codifica, limitata dalla memoria del dispositivo digitale e dalla sua velocità. Il file verrà compresso, processato o trasmesso</li>
</ul>
<p>Per un efficace trattamento dei segnali è necessario minimizzare il quantitativo dei dati processati individuando solo quelli strettamente necessari, in modo da perdere meno informazioni possibili. La differenza tra il segnale analogico e la sua rappresentazione digitale è definita rumore di quantizzazione; il rumore diminuisce all’aumentare dei bit richiesti per la codifica del singolo campione.</p>
<p>Per <em>Ampiezza</em> si intende il valore assunto dal segnale e sarà la variabile dipendente <span class="math inline">\(y\)</span>. Il tempo (o lo spazio) corrisponde alla variabile indipendente <span class="math inline">\(x\)</span>, monodimensionale o a più dimensioni.</p>
<p>Le grandezze statistiche utilizzate sono:</p>
<p>Energy: <span class="math inline">\(E_f = \int_{-\infty}^{+\infty} f^2(t) dt\)</span></p>
<p>Power: <span class="math inline">\(P_f = \lim_{T \rightarrow \infty} \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}} (|f(t)|)^2 dt\)</span></p>
<p>Average: <span class="math inline">\(\mu = \lim_{T \rightarrow \infty} \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}} x(t) dt = \frac{1}{T_1 - T_0}\int_{T_0}^{T_1} x(t) dt\)</span></p>
<p>Se il segnale è una forma d’onda ripetuta, queste escursioni sono costanti e possono essere descritte da una grandezza chiamata Ampiezza <em>Picco-Picco</em>.</p>
<p>La periodicità di un segnale indica il tempo nel quale è definito il segnale che si ripete. La frequenza fondamentale è legata al periodo della relazione <span class="math inline">\(f_0 = 1/T\)</span>. Nella realtà non esistono segnali puramente periodici, ma segnali quasi periodici caratterizzati da forme d’onda che si ripetono quasi uguali.</p>
<p>Il <em>Decibel</em> è un’unità di misura di tipo logaritmico che esprime il rapporto fra due livelli di potenza. La misura in decibel tra due grandezze fisiche dello stesso tipo è quindi una misura relativa, adimensionale e non lineare:</p>
<p><span class="math display">\[Bel = \log_{10}\frac{P_1}{P_2} \implies dB = 10Bel = 20\log_{10}\frac{A_1}{A_2}\]</span></p>
<!-- Lecture 2: 06/10/2020 -->
</div>
<div id="analisi-di-fourier" class="section level1">
<h1><span class="header-section-number">2</span> Analisi di Fourier</h1>
<p>L’Analisi di Fourier ha lo scopo di decomporre il segnale in costituenti sinusoidali di frequenze differenti. In particolare, consente di osservare il segnale non più nel dominio tempo - spazio, ma nel dominio delle frequenze.</p>
<p>Ogni funzione periodica e a quadrato sommabile può essere espressa come somma di funzioni di seno e coseno:</p>
<p><span class="math display">\[y = A \sin(\overline \omega x + \phi) \\
y = A \cos(\overline \omega x + \phi)\]</span></p>
<p>L’ampiezza indica quali sono i valori che la sinusoide può assumere, La pulsazione (<span class="math inline">\(\overline \omega = 2 \pi / T\)</span>) indica la frequenza della sinusoide, la fase indica quanto ritardo c’è nella sinusoide classica</p>
<!-- Mettere Foto -->
<p>Ad esempio, il segnale della immagine di sinistra è ottenuto sommando i segnali nella parte destra.</p>
<p><img src="Immagini/Somma-Segnali.png" width="100%" style="display: block; margin: auto;" /></p>
<p>La serie di Furier scrive un segnale nella seguente forma:</p>
<p><span class="math display">\[f(x) = \frac{a_0}{2} + \sum_{k = 1}^{\infty} a_k \cos(\frac{2 \pi}{N} kx) + b_k \sin(\frac{2 \pi}{N} kx)\]</span></p>
<p>con <span class="math inline">\(N\)</span> periodo, (<span class="math inline">\(1/N\)</span>) definita come frequenza fondamentale <span class="math inline">\(f_0\)</span>, <span class="math inline">\(k/n\)</span> frequenza <span class="math inline">\(f_k = kf_0\)</span> e <span class="math inline">\(\overline \omega = 2 \pi f_k\)</span> pulsazione. La formula diventa perciò:</p>
<p><span class="math display">\[f(x) = \frac{a_0}{2} + \sum_{k = 1}^{\infty} a_k \cos(2 \pi k f_0 x) + b_k \sin(2 \pi k f_0 x)\]</span></p>
<p>Data la funzione <span class="math inline">\(f(x)\)</span> periodica, i coefficienti della serie sono univocamente determinati:</p>
<p><span class="math display">\[\begin{aligned}
    a_k = \frac{2}{N} \int_{-N/2}^{N/2} f(x) \cos(2 \pi k f_0 x) \\
    b_k = \frac{2}{N} \int_{-N/2}^{N/2} f(x) \sin(2 \pi k f_0 x) \\
  \end{aligned}\]</span></p>
<div id="trasformata" class="section level2">
<h2><span class="header-section-number">2.1</span> Trasformata</h2>
<p>Ogni funzione continua <span class="math inline">\(f(x)\)</span>, anche se non periodica, può essere espressa come integrale di sinusoidi complesse opportunamente pesate:</p>
<p><span class="math display">\[F(u) = \int_{-\infty}^{+\infty} f(x) e^{-j2 \pi u x} dx \\
  f(x) = \int_{-\infty}^{+\infty} F(u) e^{j2 \pi u x} dx \]</span></p>
<p>Inoltre, è possibile passare dalla trasformata di Fourier, definita come il dominio delle frequenze (o trasformato), alla anti-trasformata di Fourier, definita come dominio temporale. Questa trasformazione avviene senza perdita di informazione.</p>
<p>La trasformata di Fourier di una funzione continua ed integrabile è una funzione complessa nel dominio delle frequenze. In coordinate polari si ha:</p>
<p><span class="math display">\[F(u) = F[f(x)] = \Re(u) + j\Im(u) = |F(u)|e^{j \phi(u)}\]</span></p>
<p>Il modulo della trasformata <span class="math inline">\(|F(u)|\)</span> è definito come:</p>
<p><span class="math display">\[|F(u)| = \sqrt{\Re(u)^2+ \Im(u)^2}\]</span></p>
<p>mentre la fase <span class="math inline">\(\phi(u)\)</span>:</p>
<p><span class="math display">\[\phi(u) = \tan^{-1} \frac{\Im(u)}{\Re(u)}\]</span></p>
<p>Per il caso bidimensionale l’equazione della trasformata assume un’altra forma:</p>
<p><span class="math display">\[\begin{aligned}
    F(u, v) &amp;= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x, y) \cdot e^{-i2\pi(ux + vy)} dx dy \\
    f(u, v) &amp;= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}F(x, y) \cdot e^{i2\pi(ux + vy)} dx dy
  \end{aligned}\]</span></p>
<p>che rimane molto simile a parte per il numero di assi.</p>
<p>Operando con segnali digitali, che assumono valori discreti, l’integrale è sostituito dalla sommatoria:</p>
<p><span class="math display">\[F(u) = \frac{1}{N} \sum_{i = 0}^{N - 1}f(j)e^{-j 2 \pi u \frac{1}{N}i}\]</span></p>
<p>Si intende filtrare il suono di un’onda eliminando le frequenze sopra una certa soglia (ad esempio conservando solamente i bassi). Si effettua questa operazione nello spazio delle frequenze, molto più semplice in quanto si programma un filtro di una funzione che vale 0 sulle frequenze da eliminare e 1 quelle da conservare, e lo si moltiplica per la trasformata <span class="math inline">\(F(u)\)</span>, effettuando l’anti-trasformata per poter fruire nuovamente del file.</p>
<p>Si può effettuare un filtraggio anche con le immagini, considerando solamente il modulo della trasformata bidimensionale del segnale. Più la rappresentazione del modulo è regolare, più l’immagine sarà ordinata.
Per costruire un filtro basta considerare una circonferenza di raggio arbitrario per considerare solamente determinate frequenze.
Considerando solamente le frequenze alte sono conservati i bordi dell’immagine, con frequenze basse le informazioni sul contenuto (si ottiene una sfocatura dell’immagine).</p>
<p><img src="Immagini/Spazio-Temporale-Frequenze.png" width="100%" style="display: block; margin: auto;" /></p>
<!-- Lecture 4: 13/10/2020 -->
</div>
<div id="convoluzioni" class="section level2">
<h2><span class="header-section-number">2.2</span> Convoluzioni</h2>
<p>Una <em>Convoluzione</em> è l’operatore che descrive i filtraggi lineari nel dominio spaziale. In particolare, è l’applicazione di un filtraggio <span class="math inline">\(g\)</span> ad una funzione <span class="math inline">\(f\)</span>. Si considera una convoluzione nel dominio continuo tra due funzioni <span class="math inline">\(f(x)\)</span> e <span class="math inline">\(g(x)\)</span>:</p>
<p><span class="math display">\[(f \ast g)(x) = \int_{s = -\infty}^{+\infty}g(x - s)f(s) ds\]</span></p>
<p>In particolare,</p>
<ol style="list-style-type: decimal">
<li>L’asse di rappresentazione di uno dei due segnali è invertita: <span class="math inline">\(g(t) \rightarrow g(-t)\)</span>;</li>
<li>Il segnale invertito viene traslato tra <span class="math inline">\(-\infty\)</span> e <span class="math inline">\(+\infty\)</span>;</li>
<li>Per ogni traslazione si calcola il prodotto del segnale traslato e quello non traslato;</li>
<li>Si calcola l’area del prodotto.</li>
</ol>
<!-- Esempio Convoluzione -->
<p>I filtri possono essere a media mobile, in inglese smoothing, nella quale i coefficienti sommano ad 1 (<span class="math inline">\(\sum_i c_i = 1\)</span>), e derivativi, nella quale i coefficienti sommano a 0 (<span class="math inline">\(\sum_i c_i = 0\)</span>)</p>
<p>Applicando la definizione di Trasformata di Fourier è possibile dimostrare il Teorema della Convoluzione:</p>
<p>La trasformata della convoluzione di due funzioni è il prodotto delle trasformate delle stesse:</p>
<p><span class="math display">\[G(u) = F[g(x)] = F[f(x) \ast h(x)]= F(u)H(u)\]</span></p>
<p>Per la corrispondenza fra dominio spaziale e dominio delle frequenze si hanno le seguenti relazioni:</p>
<p><span class="math display">\[\begin{matrix}
    \text{Dominio Spaziale} &amp;&amp;                 &amp;&amp; \text{Dominio delle Frequenze} \\
    g(x) = f(x) \ast h(x)   &amp;&amp; \Leftrightarrow &amp;&amp; G(u) = F(u) H(u) \\
    g(x) = f(x)h(x)         &amp;&amp; \Leftrightarrow &amp;&amp; G(u) = F(u) \ast H(u) 
  \end{matrix}\]</span></p>
</div>
</div>
<div id="fondamenti-di-immagini-digitali" class="section level1">
<h1><span class="header-section-number">3</span> Fondamenti di Immagini Digitali</h1>
<p>Una <em>Immagine</em> è una funzione di intensità di luce a due dimensioni, <span class="math inline">\(f(x, y)\)</span>, con coordinate spaziali rispetto alla luce in quel punto. Una <em>Immagine Digitale</em>, invece, è una rappresentazione di una immagine continua tramite un array bidimensionale di carattere discreto. Ogni elemento dell’array campionato è chiamato <em>Pixel</em>.</p>
<p>Il <em>Livello di Grigio</em> di una immagine è l’intensità relativa per ogni unità d’area, di solito compresa tra il valore più basso d’intensità (Nero = 0) e più intenso (Bianco = 255).</p>
<p>L’<em>Intensità</em> di una immagine è l’energia di luce, emessa da una unità d’area nell’immagine (dipende dal dispositivo), mentre la <em>Luminosità</em> di una immagine è l’apparenza soggettiva di una unità d’area (soggettiva e dipende dal contesto).</p>
<!-- Luminance Vs Lightness -->
<p>La <em>Luminance</em> è definita come potenza della luce pesata per una funzione spettrale chiamata efficienza luminosa. Essa mi</p>
<p>I pixel <span class="math inline">\(f(x,y) = f_{yx}\)</span> sono ordinati in modo naturale in una matrice, con <span class="math inline">\(x\)</span> la colonna e <span class="math inline">\(y\)</span> l’indice di riga:</p>
<p><span class="math display">\[\begin{bmatrix}
    f(0,0)   &amp; f(1,0)   &amp; \dots &amp; f(N-1, L-0) \\
    f(0,1)   &amp; f(1,1)   &amp; \dots &amp; f(N-1, 1)   \\
    \vdots   &amp; \vdots   &amp;       &amp; \vdots      \\
    f(0,L-1) &amp; f(1,L-1) &amp; \dots &amp; f(N-1, L-1) \\
  \end{bmatrix}\]</span></p>
<!-- Vedere Trasformazioni Immagini -->
<p>Il <em>Contrasto</em> di un punto di un’immagine è definito come la differenza relativa tra l’intensità del punto stesso e quella del suo vicino:</p>
<p><span class="math display">\[C = \frac{I_p - I_n}{I_n}\]</span></p>
<p>Il contrasto di una immagine intera, invece, è definito come la quantità di livello di grigio presente nella stessa. Per osservare il livello di grigio presente in una foto è necessario costruire un istogramma, che descrive la relativa proporzione di livello di grigio (assoluta, normalizzata e cumulata). è possibile manipolare il contrasto della intera immagine con lo <em>Stretching</em>, diminuendo il contrasto di un immagine, e <em>Shifting</em>, aumentando il valore di grigi.</p>
<!-- Lecture 6: 20/10/2020 -->
<div id="miglioramento-immagini" class="section level2">
<h2><span class="header-section-number">3.1</span> Miglioramento Immagini</h2>
<p>Il miglioramento delle immagini è il processo nella quale una immagine viene visualizzata meglio, ma che dipende dal tipo di problema da svolgere. Esso può essere effettuato sia nel dominio dello spazio, lavorando sulla immagine originale, che in quello delle frequenze, operando sulla trasformata di Fourier.</p>
<p><span class="math display">\[g(x, y) = T[f(x, y)]\]</span></p>
<p>Esistono diverse tecniche di miglioramento delle immagini:</p>
<ul>
<li><em>Point Operations</em>, nella quale qualsiasi punto di una immagine dipende solamente dal livello di grigio in quel punto. Le operazioni più utilizzate sono:
<ul>
<li>Contrast Stretching, nella quale si aumenta il range dinamico di dei livelli di grigio nella immagine. I valori prima di una soglia <span class="math inline">\(m\)</span> verranno compressi con una trasformazione <span class="math inline">\(s = T(r)\)</span>;</li>
<li>Image Negatives, ovvero una trasformazione negativa della seguente espressione: <span class="math inline">\(s = L - 1 - r\)</span>;</li>
<li>Compression of Dynamic Range, in quanto alcune volte è meglio comprimere i valori di grigio di una immagine. Di solito si utilizza una trasformazione logaritmica: <span class="math inline">\(T(r) = c \log(1 + |r|)\)</span>, altre volte si utilizza la trasformazione <span class="math inline">\(s = cr^{\gamma}\)</span>. Il fenomeno di correzione questa equazione è chiamata Gamma Correction;</li>
<li>Gray-Level Slicing, nella quale si focalizza l’attenzione su uno specifico range di valori di grigio. In questo caso è possibile evidenziare il range con un alto contrasto il range desiderato (Binarizzazione), oppure preservare la tonalità dello sfondo.</li>
<li>Bit Plane;</li>
<li>Histogram Operations, nella quale è possibile modificare il livello medio dell’immagine (<em>Sliding</em>), espandere/comprimere il range dinamico della scala di grigi (<em>Stretching/Shrinking</em>), o migliorare il contrasto di una immagine distribuendo equamente il livello di grigio di una foto.</li>
<li>Local Enhancement, nella quale scegliendo un intorno quadrato, si compie un istogramma locale e si applica l’equalizzatore a partire dal pixel centrale;</li>
</ul></li>
<li>Mask Operation, un filtraggio lineare di una immagine <span class="math inline">\(f\)</span> che forma una sotto-immagine:</li>
</ul>
<p><span class="math display">\[g(x, y) = \sum_{s = -a}^a \sum_{s = -b}^b w(s,t) f(x + s, y + t)\]</span></p>
<p>Lo Smoothing è una operazione di filtraggio nella quale si sfuma l’immagine riducendo la variazione dei pixel in scala di grigi.</p>
</div>
</div>
<div id="color-spaces" class="section level1">
<h1><span class="header-section-number">4</span> Color Spaces</h1>
<p>Un <em>Modello di Colore</em> è un modello matematico astratto che descrive come i colori possono essere rappresentati tramite numeri, tipicamente composto da 3 o 4 componenti. Lo <em>Spazio di Colore</em> è un assortimento di 3 dimensioni di colori nella quale ogni colore è rappresentato tramite un punti. Ne esistono di diversi tipi: il più importante è chiamato Device Derived, utilizzato per descrivere il livello del display del dispositivo, utilizzando il colore <em>RGB</em>.</p>
<p>I colori possono essere additivi, a partire dal livello di colore nero ed aggiungere i colori primari (rosso, verde e blu), oppure sottrattivo, a partire dal bianco e sottrarre i complementi dei colori primari (ciano, magenta e giallo).</p>
<p>I colori in definitiva possono essere definiti come una combinazione dei colori primari:</p>
<p><span class="math display">\[\text{Color} = r\text{*R*} + g\text{*G*} + b\text{*B*} = 
                \begin{bmatrix}
                  \text{*R*} &amp; \text{*G*} &amp; \text{*B*}
                \end{bmatrix}
                \begin{bmatrix}
                  r \\ g \\ b
                \end{bmatrix}\]</span></p>
<p>Il modello <em>RGB</em> è Device Oriented, ovvero la rappresentazione dei colori dipende dal tipo di dispositivo e riguarda sia l’acquisizione, ovvero il valore del colore dipende dalla sensibilità spettrale del sensore della camera, che l’esposizione, ovvero il colore RGB appare differente se visto da un altro dispositiva.</p>
<p>Molte volte è consigliato normalizzare i valori:</p>
<p><span class="math display">\[\begin{aligned}
    I = \frac{R + G + B}{3} \qquad r = \frac{R}{R + G + B}
    g = \frac{G}{R + G + B} \qquad b = \frac{B}{R + G + B}
\end{aligned}\]</span></p>
<p><img src="Immagini/RGB.png" width="100%" style="display: block; margin: auto;" /></p>
<p>L’acquisizione delle immagini da parte di un dispositivo applica una Gamma correction per trasformare la luce da termini lineari a non linear, mentre i display applicano l’operazione inversa.</p>
<!-- Lecture 8: 27/10/2020 -->
<p>Un altro spazio di colori molto famoso è chiamato <em>Intuitive</em>, basato sulla descrizione dei colori familiari. Il modello di colore associato è <strong><em>H</em></strong>ue <strong><em>S</em></strong>aturation <strong><em>I</em></strong>ntensity (<strong>HSV</strong>):</p>
<ul>
<li><em>Tonalità</em>: indica la posizione del colore;</li>
<li><em>Saturazione</em>: indica le coordinate radiali della ruota dei colori</li>
<li><em>Intensità</em>: indica la quantità di luce.</li>
</ul>
<p>Questo spazio di colore è molto famoso in quanto più intuitivo e permette di compiere delle trasformazioni più efficienti rispetto dello spazio di colore precedente. Il problema principale è che quando una delle tre componenti è circa 0, le altre componenti diventano instabili.</p>
<p><img src="Immagini/HSV.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Un altro modello molto utilizzato è il modello di colori <em>CMY</em>, spesso utilizzato per stampare fogli. Esso è il complemento del modello RGB: Al posto di aggiungere colori al nero, si sottraggono i colori dal bianco.</p>
<ul>
<li><em>Ciano</em>, controlla la quantità di colore rosso nella stampante;</li>
<li><em>Magenta</em>, controlla la quantità di colore verde;</li>
<li><em>Giallo</em>, controlla la quantità di colore blu.</li>
</ul>
<p><span class="math display">\[\begin{bmatrix}
    C \\ M \\ Y
  \end{bmatrix} =
  \begin{bmatrix}
    1 \\ 1 \\ 1 
  \end{bmatrix} -
  \begin{bmatrix}
    R \\ G \\ B
  \end{bmatrix}\]</span></p>
<p><img src="Immagini/CMY.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="interest-point-detectors-and-descriptors" class="section level1">
<h1><span class="header-section-number">5</span> Interest Point Detectors and Descriptors</h1>
<p>Uno degli aspetti principali della Data Science è riconoscere oggetti presenti nelle immagini. In particolare, si chiama <em>Object Recognition</em> un classe di oggetti da dover identificare, mentre <em>Istance Recognition</em> una particolare istanza della classe dell’oggetto.</p>
<p>Un modo intuitivo per riconoscere un oggetto all’interno di una immagine è individuare tramite la rotazione, traslazione, illuminazione e qualità della immagine in inglese chiamato <em>Template Matching</em>. In particolare, è possibile utilizzare due approcci:</p>
<ul>
<li><strong><em>S</em></strong>um of <strong><em>S</em></strong>quared <strong><em>D</em></strong>ifferences (<strong>SSD</strong>), un metodo molto veloce, ma sensibile alla intensità generale della immagine;</li>
<li><em>Normalized Cross Correlation</em>, più lenta della precedente, ma non varia al variare della intensità di luce locale e al contrasto.</li>
</ul>
<div id="local-descriptors-trends" class="section level2">
<h2><span class="header-section-number">5.1</span> Local Descriptors Trends</h2>
<p>I <em>Local Descriptor</em> sono dei descrittori che permetto di estrarre delle feature all’interno delle immagini a livello locale. Il più importante e famoso descrittore è lo <strong><em>S</em></strong>cale <strong><em>I</em></strong>nvariant <strong><em>F</em></strong>eature <strong><em>T</em></strong>ransform (<strong>SIFT</strong>), che permette di individuare oggetti anche a scale diverse all’interno della scena.</p>
<p>I descrittori locali sono <em>Keypoints-based</em> in quanto sono metodi efficienti per applicazioni in tempo reale. Sono in grado di comprimere informazioni in immagini di grande dimensione e riconoscere parti specifiche delle foto. Inoltre, gli algoritmi non devono compiere nuovi addestramenti (<em>traininig</em>) ogni volta che si applica la funzione.</p>
<p>I punti più importanti all’interno di una immagine sono chiamati <em>Corner</em>, in quanto si verificano i maggiori cambiamenti in termini di luce e colore al loro esterno. La funzione che identifica questi punti è chiamata <em>Harris Corner Detector</em>, deinifita come la differenza tra l’immagine originale e la finestra in movimento tra le coordinate <span class="math inline">\(x, y\)</span>:</p>
<p><span class="math display">\[E(u, v) = \sum_{x, y}w(x, y)[I(x + u, y + v) - I(x, y)]^2\]</span></p>
<p>che grazie alla espansione di Taylor è possibile scriverla nella seguente formula matriciale:</p>
<p><span class="math display">\[M = \underset{(x,y)\in W}\sum 
    \begin{bmatrix}
      I_x^2 &amp; I_x I_y \\
      I_x I_y &amp; I_y^2 
    \end{bmatrix}\]</span></p>
<p>Gli autovalori della matrice sono utili per determinare il vero cambiamento del punto:</p>
<p><img src="Immagini/Harris-Corner-Detector.jpg" width="100%" style="display: block; margin: auto;" /></p>
<p>I passaggi fondamentali per la costruzione di un descrittore sono:
<!-- Anche SIFT, quindi se domanda chiede i passaggi usare questa --></p>
<ol style="list-style-type: decimal">
<li><em>Key-point Detection</em>: si costruisce lo Scale Space in modo tale da ottenere l’invarianza di scala (alto valore = alto blur, ma meno dettagli). L’immagine viene blurrata con dei filtri Gaussiani di intensità sempre maggiori, utilizzando diverse a scale e a diverse ottave. Queste immagini filtrate vengono sottratte una con l’altra per ottenere l’immagine differenza In questo modo si identificano i massimi e minimi dei punti. Esso è un candidato Keypoint se ha il valore più alto di tutto il suo vicinato. Si eliminano i candidati a basso contrasto, sotto il valore di una certa soglia, mantenendo solamente i valori con alto contrasto sia in termini verticali che orizzontali. Infine, si calcola l’orientazione dei punti per ottenere una invarianza di rotazioni, calcolando magnitudine ed orientazione dei punti. Verrà costruito un istogramma identificando il valore massima di orientazione;</li>
<li><em>Key-point Description</em>: si costruisce una finestra <span class="math inline">\(4 \times 4\)</span> attorno al Keypoint selezionato, in ciascuna di queste celle viene calcolato l’istogramma per determinare l’orientazione. Si ottiene una descrizione dell’intorno del keypoint ottenendo 128 valori per descrivere l’intorno del punto. Il descrittore viene diviso per la sua norma <span class="math inline">\(L2\)</span> e viene calcolata l’invarianza per rotazione. Tutti i valori superiori a 0.2 vengono assegnati con il valore stesso e successivamente il vettore viene rinormalizzato;</li>
<li>Confronto dei Key-point simili (<em>Keypoint Matching</em>): i punti descrittori delle due immagini vengono confrontati utilizzando la distanza euclidea;</li>
<li>Punteggio di similarità basato sui punti di matching (<em>Score</em>): per confrontare la similarità tra due immagini è necessario mettere le immagini sullo stesso piano (<em>Omografia</em>). il metodo di confronto dei punti è chiamato <strong><em>RAN</em></strong>dom <strong><em>SA</em></strong>mple <strong><em>C</em></strong>onsensus (<strong>RANSAC</strong>). Si seleziona la funzione che meglio approssima i punti, eliminando quelli più lontani perchè potenzialmente outlier. Il punteggio di score è definito come il rapporto delle distanze con il primo ed il secondo match identificato e si sommano tutti i match identificati. Il valore ha una tolleranza imposta dall’utente e se la supera allora le due immagini contengono lo stesso oggetto.</li>
</ol>
<p>La metrica Metriche di valutazione più importante è chiamata <em>Intersection of Union</em>, definita come il rapporto tra l’area di intersezione e di unione di due immagini. Si inserisce una soglia al valore calcolato e per ciascun punto si identificherà un <span class="math inline">\(TP, TN, FP\)</span> o <span class="math inline">\(FN\)</span>.</p>
<p><span class="math display">\[IoU = \frac{\text{Area di Intersezione}}{\text{Area di Unione}}\]</span></p>
<!-- Lecture 10: 03/11/2020 -->
</div>
</div>
<div id="trainable-classifiers" class="section level1">
<h1><span class="header-section-number">6</span> Trainable Classifiers</h1>
<p>Il paradigma per riconoscere dei pattern all’interno di immagini è il seguente: a partire da un insieme di immagini di training si estraggono le principali feature di esse. Successivamente di classificano le immagini per allenare il classificatore che verrà poi utilizzato per successive immagini.</p>
<p>1-NN è un classificatore nella quale i descittori sono visti come un insieme di punti in uno spazio vettioare k-dimensionale. Successivmanete viene scelta una funzione di distanza (Euclidea, Mahnattan) per identificare la topopogia dello spazio</p>
<div id="support-vector-machine" class="section level2">
<h2><span class="header-section-number">6.1</span> Support Vector Machine</h2>
<p>Un altro classificatore molto famoso è chiamato <strong><em>S</em></strong>upport <strong><em>V</em></strong>ector <strong><em>M</em></strong>achine (<strong>SVM</strong>), un modello che separa i valori di due (o più) classi diverse. Se i dati possono essere linearmente separabili, esistono più rette che separano le classi. In questo caso l’SVM individua il piano che massimizza il margine tra i valori delle classi.</p>
<p><span class="math display">\[w x + b = \sum_i \alpha_i y_i x_i x + b\]</span></p>
<p><img src="Immagini/SVM.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Certe volte i punti non possono essere linearmente separabili. Per risolvere questo problema, si utilizza il <em>Kernel Trick</em>: se nello spazio di partenza i dati non sono linearmente separabili, lo possono essere in uno spazio vettoriale di dimensione superiore a quello di partenza:</p>
<p><span class="math display">\[\sum_{i = 1}^n \alpha_iy_i \phi(x_i)\phi(x) + b = \sum_{i = 1}^n \alpha_iy_iK(x_ix) + b\]</span></p>
</div>
<div id="neural-network" class="section level2">
<h2><span class="header-section-number">6.2</span> Neural Network</h2>
<p>Le <strong><em>N</em></strong>eural <strong><em>N</em></strong>etwork (<strong>NN</strong>) sono dei modelli molto avanzati ispirati dai neuroni biologici che costituiscono il cervello animale.</p>
<p>Negli anni ’60 Hubel e Wiesel hanno scoperto che le feature rispondono a pattern di attivazione in celle di basso livello e propagano l’attivazione a neuroni di una più alta gerarchia.</p>
<p>Un <strong><em>M</em></strong>ulti-<strong><em>L</em></strong>ayer <strong><em>P</em></strong>erceptron (*MLP**) consiste in un diverso numero neuroni artificiali che comunicano in modo unidirezionale, dalle variabili di input <span class="math inline">\(X\)</span> all’attributo di classe.
In generale, si calcola come la combinazione lineare tra le variabili di input meno la soglia (treshold):</p>
<p><span class="math display">\[y_j = f(\sum_{i = 1}^n w_{ij} \times x_i - \theta_j) = f(z_j - \theta_j)\]</span></p>
<ul>
<li>Neuroni di Input, associato alle covariate;</li>
<li>Neuroni Nascosti;</li>
<li>Neuroni di Output, associato all’attributo di classe.</li>
</ul>
<p>Ogni neurone di input è connesso in modo unidirezionale ai neuroni nascosti, propagando il segnale dal layer di input a quello nascosto. Quando tutti i neuroni del layer nascosto ricevono il segnale dai layer di input, il segnale è mandato a quello di output. Tuttavia, è possibile anche avere più di un layer nascosto: il primo layer manda tutti i segnali al secondo che si attiveranno. Alla fine, quest’ultimo manda dei segnali al neurone di output.</p>
<p>Il <em>Teorema di Approssimazione Universale</em> afferma che la rete Feed-Forward multistrato standard con un singolo layer nascosto, che contiene un numero finito di neuroni nascosti, è un approssimatore universale tra funzioni continue su sottoinsiemi compatti di <span class="math inline">\(R^n\)</span>, sotto lievi ipotesi sulla funzione di attivazione.</p>
<p>Per determinare il minimo di una funzione è possibile utilizzare il <em>Gradient Descent</em>. Si sceglie la direzione <span class="math inline">\(d_k\)</span> da seguire partendo da un punto <span class="math inline">\(x_k\)</span>; successivamente, si massimizza (o minimizza) lungo la direzione il valore al fine di trovare un nuovo punto <span class="math inline">\(x_{k+1} = x_k + \alpha_k d_k\)</span>, con <span class="math inline">\(k\)</span> il numero di iterazione e <span class="math inline">\(\alpha_k\)</span> chiamata <em>Step Size</em>. I passaggi da seguire sono:</p>
<ol style="list-style-type: decimal">
<li>Scegliere un punto iniziale <span class="math inline">\(x_0\)</span>;</li>
<li>Calcolare <span class="math inline">\(\nabla f(x_k)\)</span> alla k-esima iterazione;</li>
<li>Calcolare il vettore di ricerca <span class="math inline">\(d_k = \pm \nabla f(x_k)\)</span>;</li>
<li>Calcolare il punto successivo <span class="math inline">\(x_{k+1} = x_k \pm \alpha_k d_k\)</span>;</li>
<li>Utilizzare un metodo di risoluzione univariata per ottenere <span class="math inline">\(\alpha_k\)</span>;</li>
<li>Determinare la convergenza utilizzando una tolleranza <span class="math inline">\(|f(x_{x+1}) - f(x_k)| &lt; \varepsilon_1\)</span>, oppure <span class="math inline">\(||\nabla f(x_{k+1})|| &lt; \varepsilon_2\)</span>.</li>
</ol>
<p>In questo modo è possibile individuare i parametri che minimizzano la Loss Function del modello. Si individuano i pesi della rete che minimizzano il valore atteso e quello previsto dal training:</p>
<p><span class="math display">\[L(v) = \sum_{j = 1}^n [(y_j - f_w(x_j)]^2\]</span></p>
<p>Di solito si aggiunge un valore di penalizzazione sui pesi della funzione obiettivo:</p>
<p><span class="math display">\[L(v) = \sum_{j = 1}^n [(y_j - f_w(x_j)]^2 + \frac{\lambda}{2} \sum_{j = 1}^n w_j^2\]</span></p>
</div>
<div id="convolutional-neural-network" class="section level2">
<h2><span class="header-section-number">6.3</span> Convolutional Neural Network</h2>
<p>Le <strong><em>C</em></strong>onvolutional <strong><em>N</em></strong>eural <strong><em>N</em></strong>etwork (<strong>CNN</strong>) sono delle Neural Network utilizzate quotidianamente nel modificare immagini aggiungendo dei filtri, ad esempio per la sfocatura di una immagine. Le CNN aiutano a ridurre i parametri focalizzandosi sulla connessione locale e costruendo i Convolutional Layer. Non tutti i neuroni sono completamente collegati, ma lo sono in un sottoinsieme nel layer successivo. Inoltre, i pesi vengono anche condivisi lungo la posizione spaziale.</p>
<!--sistemare questa parte-->
<p>Le CNN sono estrattori di feature in termini <em>feed-foward</em>, ovvero da un input di osservazioni si produce l’output. Inoltre, vengono trainate in maniera supervisionata sfruttando filtri convoluzioni ed utilizzando la Back Propagation sugli errori. Le CNN presentano i seguenti layer:</p>
<ul>
<li><em>Layer Convoluzionale</em>, dei filtri lineari, locali (porziona l’immagine comprimendola), invarianti per traslazione. Vengono usati molti filtri per ottenere una rapresentazione più ricca dei dati. Così facendo si produce una nuova mappa di attivazione tramite la banca di filtri:</li>
</ul>
<p><span class="math display">\[Y_{ijq} = y_q + \sum_{u = 0}^{H - 1}\sum_{v = 0}^{W - 1}\sum_{k = 1}^{K} x_{u + i, v + j, k} F_{u, v, k, q}\]</span></p>
<ul>
<li><em>Funzione di Attivazione</em>: dopo la convoluzione, vengono applicate le funzioni di attivazione (gating) per ottenere la mappa in uscita. Ne esistono di diverso tipo: la più importante è la sigmoide:</li>
</ul>
<p><span class="math display">\[y = \frac{1}{1 + e^{-x}}\]</span></p>
<ul>
<li><em>Layer Pooling Spaziale</em>: per ridurre il numero di parametri dei modelli, è necessario ridurre il numero di parametri applicando la media (o il massimo) di una feature in un vicinato applicato canale per canale. In questo modo si riduce anche la invarianza per traslazione aumentandone la robustezza;</li>
</ul>
<!--Rivedere LRN-->
<ul>
<li><em>Local Response Normalization</em>, nella quale si applica una normalizzazione del contrasto. In questo modo è possibile migliorare l’invarianza e la sparsità delle reti. L’LRN può essere applicato all’interno di un canale, normalizzando singoli canali vicini, oppure tra i canali, normalizzando i gruppi dei canali.</li>
</ul>
<!-- Lecture 12: 10/11/2020 -->
<!-- rivedere parte 1 -->
</div>
<div id="transfer-learning" class="section level2">
<h2><span class="header-section-number">6.4</span> Transfer Learning</h2>
<p>Tutte le architutture CNN hanno milioni di parametri da trainare, in più è necessario avere un gigantesco dataset. Se non è possibile, si può compiere un pre-training dei dati con un grande dataset ed utilizzare la CNN come fine-tuning oppure come feature extraction per l’obiettivo di interesse.</p>
<p>Si suppone di voler addestare un modello, dopodichè si prende la parte iniziale della rete eliminando la parte finale e si inserisce la porzione di interesse. Durante il training, i pesi dei layer più vicini all’input non vengono conteggiati, quindi si stimano meno i parametri.</p>
<!-- Lecture 14: 17/11/2020 -->
<!-- Lecture 16: 24/11/2020 -->
</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
