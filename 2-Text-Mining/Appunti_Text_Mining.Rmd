---
title: "Text Mining and Search"
author: "Alberto Filosa"
date: "29/9/2020"
email: "a.filosa1@campus.unimib.it"
documentclass: article
lang: it
fontsize: 11pt
geometry: margin = 0.5in
classoption: twocolumn
output: 
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 3
    number_sections: true
    fig_width: 7
    fig_height: 6
    fig_caption: true
    df_print: kable
    highlight: monochrome
  bookdown::gitbook:
    keep_md: true
    css: style.css
    split_by: rmd
    config:
      toc:
        collapse: subsection
        before: |
          <li><a > Text Mining and Search </a></li>
        after: |
          <li><a href = "https://github.com/rstudio/bookdown" target = "blank">Published with bookdown</a></li>
      download: ["pdf", "epub"]
      fontsettings:
        theme: white
        family: sans
        size: 2
      sharing:
        github: yes
        linkedin: yes
        twitter: no
        facebook: no
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = 'center')
```

<!-- Lecture 1: 29/09/2020 -->
# Introduction
> Text mining is a burgeoning new field that attempts to glean meaningful information from natural language text. It may be loosely characterized as the process of analyzing text to extract information that is useful for particular purposes.

*Text mining* is generally used to denote any system that analyzes large quantities of text and detects lexical or linguistic patterns extracting useful information.

It is generally used for sentiment analysis, document summarization, news and other recommendation and text analytics.

Data mining can be more characterized as the extraction of implicit, unknown, and potentially useful information from data. With text mining, the information extracted is clearly and explicit in the text.

There are several problems about Text Mining: first, document in an unstructured form are not accessible to be used by computers, many data are not well organized and natural language text contains ambiguities on a lexical, syntactical and semantic levels.

The Information Retrieval make it easier to find things on the Web because it finds useful answers in a collection.

The main tasks affected by Text Mining are:

* *Text Summarization*, that produce a condensed representation of the input text;
* *Information Retrieval*, that  identifies the most relevant documents to the query (used in Web Search Engine);
* *Content Based Recommender System*, that define the user profile and suggests the affinity of the research;
* *Text Classification*, that define categories according to their content (e.i. Sentiment Analysis);
* *Document Clustering*, an unsupervised learning in which there is no predefined classes, but only groups of document that share the similar topics.

<!-- Lecture 2: 01/10/2020 -->
## Basic Text Processing
*Basic Text Processing* is often the first step in any text mining application. It consists of several layers of simple processing such as tokenization, lemmatization or normalization. The purpose of analyzing texts can be either predictive or exploratory.

*Predictive Analysis* develope computer programs that automatically detect a particular concept within a span of text. Main examples are:

* Opinion Mining automatically detect if a sentence is positive or negative;
* Sentiment Analysis automatically detect the emotional state of the author in a text;
* Bias detection automatically detect if an author favors a particular viewpoint;
* Information Extraction automatically detect that a short sequence of words belongs to a particularly entity type;
* Relation Learning automatically detect pairs of entities that share a particular relation;
* Text Driven Forecasting monitors incoming text and making prediction about external events or trends;
* Temporal Summarization monitors incoming text about a news event and predicting whether a sentence should be included in an on-going summary of the event.

*Exploratory Analysis* develop computer programs that automatically discover interesting and useful patterns in text collections.

A document is a list of text, structure, other media such as images and sounds, and metadata. *Metadata* associated with a document is data related to the document and it consists of descriptive, related to the creation of the document, and semantic metadata, relate to the subject dealt with in the document.

Metadata is information about information, cataloging and descriptive information structured to allow pages to be properly searched by computers.

<!-- Lecture 3: 05/10/2020 -->
# Text Processing
The aim of *Text Processing* is extract and identifying the corpus of the text. When defining a collection, it is necessary read the document, so make it processable and understand the worlds in a semantic level. 
Terms are the basic features of text and they collected in a bag-of-world.

*Tokenization* is the process of demarcating and possibly classifying sections of a string of input characters. A token is an instance of a sequence of characters, so each of these is a candidate to be a meaningful term after further processing. There are many problems about this procedure: firstly, how split a city name like  `San Francisco` or `Los Angeles`; secondly, there are some problem tokenizing hyphenated sequence; numbers, recognized in different terms, and language issues (like German or Arabic language). Basically, the tokenizations consists in breaking a stream of text into meaningful units that depends on language, corpus or even context.

`It's not straight-forward to perform so-called "tokenization"`

`It`, `’`, `s`, `not`, `straight`, `-`, `forward`
`to`, `perform`, `so`, `-`, `called`, `“`, `tokenization`, `.`

To solve these problem is useful use Regular Expression or Statistical Method to detect the token in a specific sentence. They explore rich feature to decide where the boundary of a word is.

A ***Stop Word*** is a word, usually one of a series in a stop list, that is to be ignored by a Search Engine (e.g. `the`, `a`, `and`, `to`, `be`, etc.). However, Web SE do not use stop lists due to the good compression techniques and needed in most of search queries, such as song titles of relational queries.

The ***Normalization*** is the process of mapping variants of a term to a single, standardized form. It may depends on the language an so is intertwined <!-- Intrecciarsi -->with language detection. In Search Engines is necessary to normalizes indexed text as well as query terms identically.

***Case folding*** is the process reducing all letters to lower case. There are some exception, such as proper noun or acronymous, but often the best choice is to lower case everything.

***Thesauri*** is the process to handle synonyms and homonyms in text sentences. It can be handled by hand-construct equivalence classes, rewrite the word to form equivalence-class terms or in Search Engines it can expand a query. 

**Soundex** is an approach which forms equivalence classes of words based on phonetic heuristic, useful for spelling mistakes.

**Lemmization** is the process to reduce variant forms to base form:

`am`, `are`, `is` $\rightarrow$ `be`

`car`, `cars`, `car's`, `cars'` $\rightarrow$ `car`

It implies doing proper reduction to dictionary headword form. A crude affix chopping is ***Stemming***, that reduce inflected or derived words to their root form:

`automate`, `automatic`, `automation` $\rightarrow$ `automat`

# Text Representation
*Text Representation* is the process to representing the text with graphical method. The simplest way is the binary term-document weighting, in which each document can be represented by a set of term (or binary vector) $\in \{ 0, 1\}$. Another graphical method is the count matrix, that consider the number of occurrences of a term in a document, but this not consider the ordering of words in a document.

In natural language, there are few frequent terms and terms; the *Zipf's Law* describes the frequency, $f(w)$, of an event in a set according to its rank, approximately constant (usually in an increasing order). 

$$f(w) \propto \frac{1}{r(w)} = \frac{K}{r(w)}$$

In particular, head words take large position of occurrences, but they are semantically meaningless (e.g. `the`, `a`, `we`), while tail words take major portion of vocabulary, but they rarely occur in documents (e.g. `dextrosinistral`).

```{r Luhn Analysis, echo = FALSE, out.width = "100%"}
knitr::include_graphics("Immagini/Luhn's-Analysis.png")
```

According to this *Luhn's Analysis*, words do not describe document content with the same accuracy: word importance depends not only on frequency but also on position. So frequencies have to be weighted according to their position: two cut-offs are experimentally signed to discriminate very common or very rare words. Most significant words are those between the two cut-offs, those that are neither common nor rare. It automatically generates a document specific stop list, the most frequent words.

Weights are assigned according to corpus-wise, carrying information about the document content, and document-wise, so not all terms are equally important.

The *Term Frequency* $tf_{t, d}$} represent the frequency of the term $t$ in the document $d$, often normalized by document length:

$$w_{t, d} = \frac{tf_{t, d}}{|d|}$$

To prevent bias towards longer documents, it doesn't consider the document length but the frequency of the most occurring word in the document:

$$w_{t, d} = \frac{tf_{t, d}}{\displaystyle \max_{t_i \in d} \{tf_{t_i, d}\}}$$

The *Inverse Document Frequency* $idf_t$ represents the inverse of the informativeness of the document for a term $t$:

$$idf_t = \log \Big( \frac{N}{df_t} \Big)$$

$df_t$ is the number of documents that contains $t$ and $N$ is the total number of documents.

The *Weights*, called $tf$-$idf$ weights, are the product of the two indices:

$$w_{t, d} = \frac{tf_{t, d}}{\displaystyle \max_{t_i \in d} \{ tf_{t_i, d} \}} \cdot \log \Big(\frac{N}{df_t} \Big)$$

The weight $w$ increases with the number of occurrences within a document and with the rarity of the term in the collection.

<!-- Lecture 5 15/10/2020 -->
## Natural Language Processing
Text representation can be enriched using ***N***atural ***L***anguage ***P***rocessing (**NPL**), which provides models that go deeper to uncover the meaning. A real problem about Text processing is how phrases are recognized. There are 3 possible solutions:

* Use word N-grams;
* Identify the syntactic role of words within phrases using a Part-Of-Speech tagger;
* Store word positions in indexes and use proximity operators in queries.

### Part Of Speech Tagging
The ***P***art ***O***f ***S***peech (**POS**) Tagging is a procedure that assigns to each word its syntax role in a sentence, such as nouns, verbs and adverbs. There are many applications:

* Parsing;
* Word prediction in speech recognition;
* Machine Translation;

The POS Tagging problem is determine the POS tag for a particular instance of a word, because words often have more than one tag, like `back` (*the back door*, *on my back*). It is possible to assume a conditional probability of words $\mathax{P}(w_n | w_{n - 1})$ or POS likelihood $\mathax{P}(t_n | t_{n - 1})$ to disambiguate sentences and assessing the likelihood.

1. Start with a dictionary;
2. Assign all possible tags to word from the dictionary;
3. Write rules to selectively remove tags;
4. Leave the correct tag for each word.

### Named Entity Recognition
The ***N***amed ***E***ntity ***R***ecognition (**NER**) is a very important task of Information Extraction because it identifies and classifies names in text in a particular application. It involves identification of proper names in text and classification into a set of predefined categories of interest.

Category definitions are intuitively clear, but there are many gey areas caused by metonomy, for example Organization (`England won the World Cup`) versus Location (`The World Cup took place in England`).

NER has different approaches:

* *Ruled-Based*, identifying if a sequence of words can be an entity name using lexicons that categorize names (developed by Trial and Error or ML techniques);
* *Statistic-Based*, maximizing the probability of an entity using Hidden Markov Model estimated with training data and resolving ambiguity using context.

HMM describes a process as a collection of states with transitions between them. Each state is associated with a probability distribution over worlds of possible outputs. For identifying named entities, it finds sequence of labels which gives the highest probability of the sentence.

<!-- Lecture 6: 19/10/2020-->
Natural Language is designed to make human communication efficient, but people omit a lot of common sense knowledge and keep lots of ambiguities, such as Word-Level or Syntactic ambiguity. 

Usually, a message is sent by the sender and received by the recipient, who infers *Morphological* information of the words, uses lexical rules (*Syntax*) to reconstruct the original message and compare it with its knowledge to understand its *Semantic*, what is explicit, and does not change with the context, and *Pragmatic*, what is implicit or context related. It is the study of the relationship between language and context, fundamental to explain the understanding of the language.
In the same way, a program have to rebuild the message and compare it whit a base of knowledge (usually modeled as a graph) to understand its content (*Inference*) handling a large chunk of text (*Discourse*).

The Context is the situation in which the communicative act takes place and the set of knowledge, beliefs and the like are shared by both people. It includes at a minimum the beliefs and assumptions of the language users, relating to actions, situations and a state of knowledge and attention of those who partecipate to social interactions.

To build a computer that understands text it is necessary thus NLP Pipeline:

1. Syntactic Parsing, a grammatical analysis of a given sentence, conforming to the rules of a formal grammar;
2. Relation Extraction, identifying the relationship among named entities;
3. Logic Inference, which converts chunks of text into more formal representation.

<!-- Lecture 7: 22/10/2020 -->
# Language Models
A *Language Model* (**LM**) is a formal representation of a specific language, generating a probability distribution of the words in topics. A probability to a sequence words can be used for:
iaeQ45YB@sYwv&pell Correction;
* Speech Recognition;
* Text Categorization;
* Information Retrieval;
* Summarization.

The main task is to compute the probability of a sentence (or a sequence of words), $\mathbb{P}(W) = \mathbb{P}(w_1, w_2, \dots, w_n)$, while the related task is the probability of an upcoming word: $\mathbb{P}(w_{n + 1} | w_1, \dots, w_n)$. Any model which computes any of these procedure is called a language model.

The joint probability is computed relying on the chain rule of probability:

$$\begin{align}
    \mathbb{P}(w_1, w_2, \dots, w_n) &= \mathbb{P}(w_1)\mathbb{P}(w_2 | w_1) \dots \mathbb{P}(w_n | w_1, \dots, w_{n-1}) \\
                                     &= \prod_{i = 1}^n \mathbb{P}(w_i | w_1, \dots, w_{i-1})
  \end{align}$$

but so many product could lead the number to decay pretty fast. A solution is the Markov Assumption, an approximation of the joint probability in which considers the probability of one or two words preceding the word:

$$\mathbb{P}(w_1, w_2, \dots, w_n) \approx \prod_{i = 1}^n \mathbb{P}(w_i | w_{i - k} \dots w_{i-1}$$

In this case is possible to construct an *Unigram Language Model*, the probability distribution over the words in a language, or *N-gram Language Model*, where probabilities depend on previous words. For example a Bigram LM is the probability of a word in a sequence where considering the previous word.

The Maximum Likelihood Estimate of a Bigram LM is the following:

$$\mathbb{P}(w_i | w_{i-1}) = \frac{c(w_{i-1}, w_i)}{w_{i-1}}$$

For example, look at this repository:

```
<s> I am Sam </s>
<s> Sam I am </s>
<s> I do not like green eggs and ham </s>
```

The probability of `I` at the start of the sentence is $\mathbb{P}(I | <s> = 2/3 = 0.67$, while the probability of Sam in as the last word is $\mathbb{P}(Sam | <s> = 1/3 = 0.33$.

The evaluation of a language model is the same as most important models: the dataset is splitted in training set and test set, testing the model's performance on unseen data. The best evaluation for comparing models is compare the accuracy of those. An important metric evaluation is *perplexity*, the inverse probability of the test set normalized by number of word (minimizing perplexity is the same as maximizing probability):

$$PP(W) = \sqrt[N]{\prod_{i = 1}^N \frac{1}{\mathbb{P}(w_i | w_1 \dots w_{i-1})}}$$

<!-- Lecture 8: 29/10/2020 -->
## World Embedding
*World Embedding* indicates a set of techniques in a NLP where words from a vocabulary are mapped to vectors of real numbers. It involves a mathematical embedding from a space with many dimensions per word to a vector space with a lower dimension. This model can be used to check similarity between either or words and check topic similarity and words usage per topic (worlds can be represented as vectors too).

Each document is represented by a vector of words, with a binary representation, the frequency of the words or the weighted representation (TF-Idf).

Usually, the similarity is not computed by using term-document representation, because two words are similar if their context vectors are similar. To represent words as vectors, it is possible to build a $V \times V$ *1-hot* matrix (also known as *Local Representation*) in which each word has all values equal to 0 except for the corresponding colum, equal to 1. This is a very sparse matrix, with no information between similar words. To solve these problems, the idea is associate $k$ context dimensions representing properties of the words in the vocabulary, also known as *Distributed Representation*. Distributed vectors allow to group similar words together depending on the considered context.

The difference between Local and Distributed representation is in the matrix: in particular, in LR every term in a vocabulary $T$ is represented by a binary vector, while in DR every term is represented by a real-valued vector.

For simple scenarios it is possible create a $k$-dimensional mapping for a simple example vocabulary y manually choosing contextual dimensions that make sense. This is not a simple task, in particular manual assignment of vector would be impossibly. The *Distributional Hypothesis* aims to represent each words by some context. It is possible to use different granularities of context, such as documents and sentences.

The *Window-based Co-occurence Matrix* counts the number of times each context words co-occurs inside a windows of a particular size with the word of interest. It generates an $X = |V| \times |V|$ co-occurence matrix. The frequency is not a good representation, it is possible to use weights, TF-IDF or ***P***ointwise ***M***utual ***I***nformation (**PMI**):

$$PMI(w_1, w_2) = \log_2 \frac{\mathbb{P}(w_1, w_2)}{\mathbb{P}(w_1)\mathbb{P}(w_2)} \in (- \infty, + \infty)$$

To have better estimation, only positive estimation are taken:

$$PPMI(x, y) = \max [PMI(x,y),0]$$

$f_{i, j}$ is the number of time the word $w_i$ appears in the context $c_j$. It is used to compute these probabilities:

$$\begin{aligned}
    \mathbb{P}(w_j, c_j) &= \frac{f_{i, j}}{\sum_{i,j = 1}^{W,C} f_{i, j}}
    \mathbb{P}(w_j) &= \frac{\sum_{i = 1}^C f_{i, j}}{\sum_{i,j = 1}^{W,C} f_{i, j}}
    \mathbb{P}(c_j) &= \frac{\sum_{i = 1}^W f_{i, j}}{\sum_{i,j = 1}^{W,C} f_{i, j}}
  \end{aligned}$$

For example, look at this table:

```{r PMI}
PMI <- rbind(c(0,0,1,0,1),
             c(0,0,1,0,1),
             c(2,1,0,1,0),
             c(1,6,0,4,0))

rownames(PMI) <- c("Apricot", "Pineapple", "Digital", "Information")
colnames(PMI) <- c("Computer", "Data", "Pinch", "Result", "Sugar")

knitr::kable(PMI)
```

The $\mathbb{w_4, c_2} = 6/19 = 0.32$, $\mathbb{w_4} = 11/19 = 0.58$ and $\mathbb{c_2} = 7/19 = 0.37$. The PPMI table is:

```{r PPMI}
PPMI <- rbind(c(0.00, 0.00, 0.05, 0.00, 0.05),
              c(0.00, 0.00, 0.05, 0.00, 0.05),
              c(0.11, 0.05, 0.05, 0.05, 0.00),
              c(0.05, 0.32, 0.00, 0.021, 0.05))

rownames(PPMI) <- c("Apricot", "Pineapple", "Digital", "Information")
colnames(PPMI) <- c("Computer", "Data", "Pinch", "Result", "Sugar")

knitr::kable(PPMI)
```

Rare words and apax produce higher scores, that risks to bias the analysis: probabilities of $w$ and $c$ are modified or a $k$-smoothing is used to reduced those scores with a value $\alpha \in [0, 1]$ (usually $\alpha = 0.75$):

$$PMI_{\alpha}(w, c) = \log_2 \frac{\mathbb{P}(w, c)}{\mathbb{P}(w)\mathbb{P}_{\alpha}(c)} \in (- \infty, + \infty)
\implies
PPMI_{\alpha}(w, c) = \max [(PMI_{\alpha}(w, c), 0]$$

with

$$P_{\alpha}(c) = \frac{\text{count}(c)^{\alpha}}{\sum_c \text{count}^{\alpha}}$$

