<!DOCTYPE html>
<html lang="it" xml:lang="it">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Text Mining and Search</title>
  <meta name="description" content="Text Mining and Search" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Text Mining and Search" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Text Mining and Search" />
  
  
  

<meta name="author" content="Alberto Filosa" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a > Text Mining and Search </a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path=""><a href="#introduction"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#basic-text-processing"><i class="fa fa-check"></i><b>1.1</b> Basic Text Processing</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path=""><a href="#text-processing"><i class="fa fa-check"></i><b>2</b> Text Processing</a></li>
<li class="chapter" data-level="3" data-path=""><a href="#text-representation"><i class="fa fa-check"></i><b>3</b> Text Representation</a></li>
<li class="divider"></li>
<li><a href = "https://github.com/rstudio/bookdown" target = "blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Text Mining and Search</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Text Mining and Search</h1>
<p class="author"><em>Alberto Filosa</em></p>
<p class="date"><em>29/9/2020</em></p>
</div>
<!-- Lecture 1: 29/09/2020 -->
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<blockquote>
<p>Text mining is a burgeoning new field that attempts to glean meaningful information from natural language text. It may be loosely characterized as the process of analyzing text to extract information that is useful for particular purposes.</p>
</blockquote>
<p><em>Text mining</em> is generally used to denote any system that analyzes large quantities of text and detects lexical or linguistic patterns extracting useful information.</p>
<p>It is generally used for sentiment analysis, document summarization, news and other recommendation and text analytics.</p>
<p>Data mining can be more characterized as the extraction of implicit, unknown, and potentially useful information from data. With text mining, the information extracted is clearly and explicit in the text.</p>
<p>There are several problems about Text Mining: first, document in an unstructured form are not accessible to be used by computers, many data are not well organized and natural language text contains ambiguities on a lexical, syntactical and semantic levels.</p>
<p>The Information Retrieval make it easier to find things on the Web because it finds useful answers in a collection.</p>
<p>The main tasks affected by Text Mining are:</p>
<ul>
<li><em>Text Summarization</em>, that produce a condensed representation of the input text;</li>
<li><em>Information Retrieval</em>, that identifies the most relevant documents to the query (used in Web Search Engine);</li>
<li><em>Content Based Recommender System</em>, that define the user profile and suggests the affinity of the research;</li>
<li><em>Text Classification</em>, that define categories according to their content (e.i. Sentiment Analysis);</li>
<li><em>Document Clustering</em>, an unsupervised learning in which there is no predefined classes, but only groups of document that share the similar topics.</li>
</ul>
<!-- Lecture 2: 01/10/2020 -->
<div id="basic-text-processing" class="section level2">
<h2><span class="header-section-number">1.1</span> Basic Text Processing</h2>
<p><em>Basic Text Processing</em> is often the first step in any text mining application. It consists of several layers of simple processing such as tokenization, lemmatization or normalization. The purpose of analyzing texts can be either predictive or exploratory.</p>
<p><em>Predictive Analysis</em> develope computer programs that automatically detect a particular concept within a span of text. Main examples are:</p>
<ul>
<li>Opinion Mining automatically detect if a sentence is positive or negative;</li>
<li>Sentiment Analysis automatically detect the emotional state of the author in a text;</li>
<li>Bias detection automatically detect if an author favors a particular viewpoint;</li>
<li>Information Extraction automatically detect that a short sequence of words belongs to a particularly entity type;</li>
<li>Relation Learning automatically detect pairs of entities that share a particular relation;</li>
<li>Text Driven Forecasting monitors incoming text and making prediction about external events or trends;</li>
<li>Temporal Summarization monitors incoming text about a news event and predicting whether a sentence should be included in an on-going summary of the event.</li>
</ul>
<p><em>Exploratory Analysis</em> develop computer programs that automatically discover interesting and useful patterns in text collections.</p>
<p>A document is a list of text, structure, other media such as images and sounds, and metadata. <em>Metadata</em> associated with a document is data related to the document and it consists of descriptive, related to the creation of the document, and semantic metadata, relate to the subject dealt with in the document.</p>
<p>Metadata is information about information, cataloging and descriptive information structured to allow pages to be properly searched by computers.</p>
<!-- Lecture 3: 05/10/2020 -->
</div>
</div>
<div id="text-processing" class="section level1">
<h1><span class="header-section-number">2</span> Text Processing</h1>
<p>The aim of <em>Text Processing</em> is extract and identifying the corpus of the text. When defining a collection, it is necessary read the document, so make it processable and understand the worlds in a semantic level.
Terms are the basic features of text and they collected in a bag-of-world.</p>
<p><em>Tokenization</em> is the process of demarcating and possibly classifying sections of a string of input characters. A token is an instance of a sequence of characters, so each of these is a candidate to be a meaningful term after further processing. There are many problems about this procedure: firstly, how split a city name like <code>San Francisco</code> or <code>Los Angeles</code>; secondly, there are some problem tokenizing hyphenated sequence; numbers, recognized in different terms, and language issues (like German or Arabic language). Basically, the tokenizations consists in breaking a stream of text into meaningful units that depends on language, corpus or even context.</p>
<pre><code>It&#39;s not straight-forward to perform so-called &quot;tokenization&quot;

`It`, `’`, `s`, `not`, `straight`, `-`, `forward`
`to`, `perform`, `so`, `-`, `called`, `“`, `tokenization`, `.`</code></pre>
<p>To solve these problem is useful use Regular Expression or Statistical Method to detect the token in a specific sentence. They explore rich feature to decide where the boundary of a word is.</p>
<p>A <em>Stop Word</em> is a word, usually one of a series in a stop list, that is to be ignored by a Search Engine (e.g. <code>the</code>, <code>a</code>, <code>and</code>, <code>to</code>, <code>be</code>, etc.). However, Web SE do not use stop lists due to the good compression techniques and needed in most of search queries, such as song titles of relational queries.</p>
<p>The <em>Normalization</em> is the process of mapping variants of a term to a single, standardized form. It may depends on the language an so is intertwined <!-- Intrecciarsi -->with language detection. In Search Engines is necessary to normalizes indexed text as well as query terms identically.</p>
<p><em>Case folding</em> is the process reducing all letters to lower case. There are some exception, such as proper noun or acronymous, but often the best choice is to lower case everything.</p>
<p><em>Thesauri</em> is the process to handle synonyms and homonyms in text sentences. It can be handled by hand-construct equivalence classes, rewrite the word to form equivalence-class terms or in Search Engines it can expand a query.</p>
<p><em>Soundex</em> is an approach which forms equivalence classes of words based on phonetic heuristic, useful for spelling mistakes.</p>
<p><em>Lemmization</em> is the process to reduce variant forms to base form:</p>
<pre><code>`am`, `are`, `is` $\rightarrow$ `be`

`car`, `cars`, `car&#39;s`, `cars&#39;` $\rightarrow `car`</code></pre>
<p>It implies doing proper reduction to dictionary headword form. A crude affix chopping is <em>Stemming</em>, that reduce inflected or derived words to their root form:</p>
<pre><code>`automate`, `automatic`, `automation` $\rightarrow$ `automat`</code></pre>
</div>
<div id="text-representation" class="section level1">
<h1><span class="header-section-number">3</span> Text Representation</h1>
<p><em>Text Representation</em> is the process to representing the text with graphical method. The simplest way is the binary term-document weighting, in which each document can be represented by a set of term (or binary vector) <span class="math inline">\(\in \{ 0, 1\}\)</span>. Another graphical method is the count matrix, that consider the number of occurrences of a term in a document, but this not consider the ordering of words in a document.</p>
<p>In natural language, there are few frequent terms and terms; the <em>Zipf’s Law</em> describes the frequency, <span class="math inline">\(f(w)\)</span>, of an event in a set according to its rank, approximately constant (usually in an increasing order).</p>
<p><span class="math display">\[f(w) \propto \frac{1}{r(w)} = \frac{K}{r(w)}\]</span></p>
<p>In particular, head words take large position of occurrences, but they are semantically meaningless (e.g. <code>the</code>, <code>a</code>, <code>we</code>), while tail words take major portion of vocabulary, but they rarely occur in documents (e.g. <code>dextrosinistral</code>).</p>
<p><img src="Immagini/Luhn's-Analysis.png" width="100%" style="display: block; margin: auto;" /></p>
<p>According to this <em>Luhn’s Analysis</em>, words do not describe document content with the same accuracy: word importance depends not only on frequency but also on position. So frequencies have to be weighted according to their position: two cut-offs are experimentally signed to discriminate very common or very rare words. Most significant words are those between the two cut-offs, those that are neither common nor rare. It automatically generates a document specific stop list, the most frequent words.</p>
<p>Weights are assigned according to corpus-wise, carrying information about the document content, and document-wise, so not all terms are equally important.</p>
<p>The <em>Term Frequency</em> <span class="math inline">\(tf_{t, d}\)</span>} represent the frequency of the term <span class="math inline">\(t\)</span> in the document <span class="math inline">\(d\)</span>, often normalized by document length:</p>
<p><span class="math display">\[w_{t, d} = \frac{tf_{t, d}}{|d|}\]</span></p>
<p>To prevent bias towards longer documents, it doesn’t consider the document length but the frequency of the most occurring word in the document:</p>
<p><span class="math display">\[w_{t, d} = \frac{tf_{t, d}}{\displaystyle \max_{t_i \in d} \{tf_{t_i, d}\}}\]</span></p>
<p>The <em>Inverse Document Frequency</em> <span class="math inline">\(idf_t\)</span> represents the inverse of the informativeness of the document for a term <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[idf_t = \log \Big( \frac{N}{df_t} \Big)\]</span></p>
<p><span class="math inline">\(df_t\)</span> is the number of documents that contains <span class="math inline">\(t\)</span> and <span class="math inline">\(N\)</span> is the total number of documents.</p>
<p>The <em>Weights</em>, called <span class="math inline">\(tf\)</span>-<span class="math inline">\(idf\)</span> weights, are the product of the two indices:</p>
<p><span class="math display">\[w_{t, d} = \frac{tf_{t, d}}{\displaystyle \max_{t_i \in d} \{ tf_{t_i, d} \}} \cdot \log \Big(\frac{N}{df_t} \Big)\]</span></p>
<p>The weight <span class="math inline">\(w\)</span> increases with the number of occurrences within a document and with the rarity of the term in the collection.</p>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
