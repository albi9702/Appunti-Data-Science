<!DOCTYPE html>
<html lang="it" xml:lang="it">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Text Mining and Search</title>
  <meta name="description" content="Text Mining and Search" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Text Mining and Search" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Text Mining and Search" />
  
  
  

<meta name="author" content="Alberto Filosa" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a > Text Mining and Search </a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path=""><a href="#introduction"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#basic-text-processing"><i class="fa fa-check"></i><b>1.1</b> Basic Text Processing</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path=""><a href="#text-processing"><i class="fa fa-check"></i><b>2</b> Text Processing</a></li>
<li class="chapter" data-level="3" data-path=""><a href="#text-representation"><i class="fa fa-check"></i><b>3</b> Text Representation</a><ul>
<li class="chapter" data-level="3.1" data-path=""><a href="#natural-language-processing"><i class="fa fa-check"></i><b>3.1</b> Natural Language Processing</a><ul>
<li class="chapter" data-level="3.1.1" data-path=""><a href="#part-of-speech-tagging"><i class="fa fa-check"></i><b>3.1.1</b> Part Of Speech Tagging</a></li>
<li class="chapter" data-level="3.1.2" data-path=""><a href="#named-entity-recognition"><i class="fa fa-check"></i><b>3.1.2</b> Named Entity Recognition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path=""><a href="#language-models"><i class="fa fa-check"></i><b>4</b> Language Models</a><ul>
<li class="chapter" data-level="4.1" data-path=""><a href="#world-embedding"><i class="fa fa-check"></i><b>4.1</b> World Embedding</a><ul>
<li class="chapter" data-level="4.1.1" data-path=""><a href="#count-based-models"><i class="fa fa-check"></i><b>4.1.1</b> Count-Based Models</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path=""><a href="#predictive-models"><i class="fa fa-check"></i><b>4.2</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path=""><a href="#text-classification"><i class="fa fa-check"></i><b>5</b> Text Classification</a><ul>
<li class="chapter" data-level="5.1" data-path=""><a href="#applications"><i class="fa fa-check"></i><b>5.1</b> Applications</a></li>
<li class="chapter" data-level="5.2" data-path=""><a href="#supervised-learning"><i class="fa fa-check"></i><b>5.2</b> Supervised Learning</a></li>
<li class="chapter" data-level="5.3" data-path=""><a href="#supervised-classification"><i class="fa fa-check"></i><b>5.3</b> Supervised Classification</a></li>
<li class="chapter" data-level="5.4" data-path=""><a href="#evaluations"><i class="fa fa-check"></i><b>5.4</b> Evaluations</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path=""><a href="#text-clustering"><i class="fa fa-check"></i><b>6</b> Text Clustering</a><ul>
<li class="chapter" data-level="6.1" data-path=""><a href="#flat-clustering"><i class="fa fa-check"></i><b>6.1</b> Flat Clustering</a></li>
<li class="chapter" data-level="6.2" data-path=""><a href="#hierarchical-clustering"><i class="fa fa-check"></i><b>6.2</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="6.3" data-path=""><a href="#clustering-evaluation"><i class="fa fa-check"></i><b>6.3</b> Clustering Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path=""><a href="#topic-modeling"><i class="fa fa-check"></i><b>7</b> Topic Modeling</a><ul>
<li class="chapter" data-level="7.1" data-path=""><a href="#latent-semantic-analysis"><i class="fa fa-check"></i><b>7.1</b> Latent Semantic Analysis</a></li>
<li class="chapter" data-level="7.2" data-path=""><a href="#latent-dirichlet-analysis"><i class="fa fa-check"></i><b>7.2</b> Latent Dirichlet Analysis</a></li>
<li class="chapter" data-level="7.3" data-path=""><a href="#evaluation"><i class="fa fa-check"></i><b>7.3</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path=""><a href="#topic-classification"><i class="fa fa-check"></i><b>8</b> Topic Classification</a></li>
<li class="chapter" data-level="9" data-path=""><a href="#text-summarization"><i class="fa fa-check"></i><b>9</b> Text Summarization</a></li>
<li class="divider"></li>
<li><a href = "https://github.com/rstudio/bookdown" target = "blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Text Mining and Search</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Text Mining and Search</h1>
<p class="author"><em>Alberto Filosa</em></p>
<p class="date"><em>29/9/2020</em></p>
</div>
<!-- Lecture 1: 29/09/2020 -->
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<blockquote>
<p>Text mining is a burgeoning new field that attempts to glean meaningful information from natural language text. It may be loosely characterized as the process of analyzing text to extract information that is useful for particular purposes.</p>
</blockquote>
<p><em>Text mining</em> is generally used to denote any system that analyzes large quantities of text and detects lexical or linguistic patterns extracting useful information.</p>
<p>It is generally used for sentiment analysis, document summarization, news and other recommendation and text analytics.</p>
<p>Data mining can be more characterized as the extraction of implicit, unknown, and potentially useful information from data. With text mining, the information extracted is clearly and explicit in the text.</p>
<p>There are several problems about Text Mining: first, document in an unstructured form are not accessible to be used by computers, many data are not well organized and natural language text contains ambiguities on a lexical, syntactical and semantic levels.</p>
<p>The Information Retrieval make it easier to find things on the Web because it finds useful answers in a collection.</p>
<p>The main tasks affected by Text Mining are:</p>
<ul>
<li><em>Text Summarization</em>, that produce a condensed representation of the input text;</li>
<li><em>Information Retrieval</em>, that identifies the most relevant documents to the query (used in Web Search Engine);</li>
<li><em>Content Based Recommender System</em>, that define the user profile and suggests the affinity of the research;</li>
<li><em>Text Classification</em>, that define categories according to their content (e.i. Sentiment Analysis);</li>
<li><em>Document Clustering</em>, an unsupervised learning in which there is no predefined classes, but only groups of document that share the similar topics.</li>
</ul>
<!-- Lecture 2: 01/10/2020 -->
<div id="basic-text-processing" class="section level2">
<h2><span class="header-section-number">1.1</span> Basic Text Processing</h2>
<p><em>Basic Text Processing</em> is often the first step in any text mining application. It consists of several layers of simple processing such as tokenization, lemmatization or normalization. The purpose of analyzing texts can be either predictive or exploratory.</p>
<p><em>Predictive Analysis</em> develope computer programs that automatically detect a particular concept within a span of text. Main examples are:</p>
<ul>
<li>Opinion Mining automatically detect if a sentence is positive or negative;</li>
<li>Sentiment Analysis automatically detect the emotional state of the author in a text;</li>
<li>Bias detection automatically detect if an author favors a particular viewpoint;</li>
<li>Information Extraction automatically detect that a short sequence of words belongs to a particularly entity type;</li>
<li>Relation Learning automatically detect pairs of entities that share a particular relation;</li>
<li>Text Driven Forecasting monitors incoming text and making prediction about external events or trends;</li>
<li>Temporal Summarization monitors incoming text about a news event and predicting whether a sentence should be included in an on-going summary of the event.</li>
</ul>
<p><em>Exploratory Analysis</em> develop computer programs that automatically discover interesting and useful patterns in text collections.</p>
<p>A document is a list of text, structure, other media such as images and sounds, and metadata. <em>Metadata</em> associated with a document is data related to the document and it consists of descriptive, related to the creation of the document, and semantic metadata, relate to the subject dealt with in the document.</p>
<p>Metadata is information about information, cataloging and descriptive information structured to allow pages to be properly searched by computers.</p>
<!-- Lecture 3: 05/10/2020 -->
</div>
</div>
<div id="text-processing" class="section level1">
<h1><span class="header-section-number">2</span> Text Processing</h1>
<p>The aim of <em>Text Processing</em> is extract and identifying the corpus of the text. When defining a collection, it is necessary read the document, so make it processable and understand the worlds in a semantic level.
Terms are the basic features of text and they collected in a bag-of-world.</p>
<p><em>Tokenization</em> is the process of demarcating and possibly classifying sections of a string of input characters. A token is an instance of a sequence of characters, so each of these is a candidate to be a meaningful term after further processing. There are many problems about this procedure: firstly, how split a city name like <code>San Francisco</code> or <code>Los Angeles</code>; secondly, there are some problem tokenizing hyphenated sequence; numbers, recognized in different terms, and language issues (like German or Arabic language). Basically, the tokenizations consists in breaking a stream of text into meaningful units that depends on language, corpus or even context.</p>
<p><code>It's not straight-forward to perform so-called "tokenization"</code></p>
<p><code>It</code>, <code>’</code>, <code>s</code>, <code>not</code>, <code>straight</code>, <code>-</code>, <code>forward</code>
<code>to</code>, <code>perform</code>, <code>so</code>, <code>-</code>, <code>called</code>, <code>“</code>, <code>tokenization</code>, <code>.</code></p>
<p>To solve these problem is useful use Regular Expression or Statistical Method to detect the token in a specific sentence. They explore rich feature to decide where the boundary of a word is.</p>
<p>A <strong><em>Stop Word</em></strong> is a word, usually one of a series in a stop list, that is to be ignored by a Search Engine (e.g. <code>the</code>, <code>a</code>, <code>and</code>, <code>to</code>, <code>be</code>, etc.). However, Web SE do not use stop lists due to the good compression techniques and needed in most of search queries, such as song titles of relational queries.</p>
<p>The <strong><em>Normalization</em></strong> is the process of mapping variants of a term to a single, standardized form. It may depends on the language an so is intertwined <!-- Intrecciarsi -->with language detection. In Search Engines is necessary to normalizes indexed text as well as query terms identically.</p>
<p><strong><em>Case folding</em></strong> is the process reducing all letters to lower case. There are some exception, such as proper noun or acronymous, but often the best choice is to lower case everything.</p>
<p><strong><em>Thesauri</em></strong> is the process to handle synonyms and homonyms in text sentences. It can be handled by hand-construct equivalence classes, rewrite the word to form equivalence-class terms or in Search Engines it can expand a query.</p>
<p><strong>Soundex</strong> is an approach which forms equivalence classes of words based on phonetic heuristic, useful for spelling mistakes.</p>
<p><strong>Lemmization</strong> is the process to reduce variant forms to base form:</p>
<p><code>am</code>, <code>are</code>, <code>is</code> <span class="math inline">\(\rightarrow\)</span> <code>be</code></p>
<p><code>car</code>, <code>cars</code>, <code>car's</code>, <code>cars'</code> <span class="math inline">\(\rightarrow\)</span> <code>car</code></p>
<p>It implies doing proper reduction to dictionary headword form. A crude affix chopping is <strong><em>Stemming</em></strong>, that reduce inflected or derived words to their root form:</p>
<p><code>automate</code>, <code>automatic</code>, <code>automation</code> <span class="math inline">\(\rightarrow\)</span> <code>automat</code></p>
</div>
<div id="text-representation" class="section level1">
<h1><span class="header-section-number">3</span> Text Representation</h1>
<p><em>Text Representation</em> is the process to representing the text with graphical method. The simplest way is the binary term-document weighting, in which each document can be represented by a set of term (or binary vector) <span class="math inline">\(\in \{ 0, 1\}\)</span>. Another graphical method is the count matrix, that consider the number of occurrences of a term in a document, but this not consider the ordering of words in a document.</p>
<p>In natural language, there are few frequent terms and terms; the <em>Zipf’s Law</em> describes the frequency, <span class="math inline">\(f(w)\)</span>, of an event in a set according to its rank, approximately constant (usually in an increasing order).</p>
<p><span class="math display">\[f(w) \propto \frac{1}{r(w)} = \frac{K}{r(w)}\]</span></p>
<p>In particular, head words take large position of occurrences, but they are semantically meaningless (e.g. <code>the</code>, <code>a</code>, <code>we</code>), while tail words take major portion of vocabulary, but they rarely occur in documents (e.g. <code>dextrosinistral</code>).</p>
<p><img src="Immagini/Luhn's-Analysis.png" width="100%" style="display: block; margin: auto;" /></p>
<p>According to this <em>Luhn’s Analysis</em>, words do not describe document content with the same accuracy: word importance depends not only on frequency but also on position. So frequencies have to be weighted according to their position: two cut-offs are experimentally signed to discriminate very common or very rare words. Most significant words are those between the two cut-offs, those that are neither common nor rare. It automatically generates a document specific stop list, the most frequent words.</p>
<p>Weights are assigned according to corpus-wise, carrying information about the document content, and document-wise, so not all terms are equally important.</p>
<p>The <em>Term Frequency</em> <span class="math inline">\(tf_{t, d}\)</span>} represent the frequency of the term <span class="math inline">\(t\)</span> in the document <span class="math inline">\(d\)</span>, often normalized by document length:</p>
<p><span class="math display">\[w_{t, d} = \frac{tf_{t, d}}{|d|}\]</span></p>
<p>To prevent bias towards longer documents, it doesn’t consider the document length but the frequency of the most occurring word in the document:</p>
<p><span class="math display">\[w_{t, d} = \frac{tf_{t, d}}{\displaystyle \max_{t_i \in d} \{tf_{t_i, d}\}}\]</span></p>
<p>The <em>Inverse Document Frequency</em> <span class="math inline">\(idf_t\)</span> represents the inverse of the informativeness of the document for a term <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[idf_t = \log \Big( \frac{N}{df_t} \Big)\]</span></p>
<p><span class="math inline">\(df_t\)</span> is the number of documents that contains <span class="math inline">\(t\)</span> and <span class="math inline">\(N\)</span> is the total number of documents.</p>
<p>The <em>Weights</em>, called <span class="math inline">\(tf\)</span>-<span class="math inline">\(idf\)</span> weights, are the product of the two indices:</p>
<p><span class="math display">\[w_{t, d} = \frac{tf_{t, d}}{\displaystyle \max_{t_i \in d} \{ tf_{t_i, d} \}} \cdot \log \Big(\frac{N}{df_t} \Big)\]</span></p>
<p>The weight <span class="math inline">\(w\)</span> increases with the number of occurrences within a document and with the rarity of the term in the collection.</p>
<!-- Lecture 5 15/10/2020 -->
<div id="natural-language-processing" class="section level2">
<h2><span class="header-section-number">3.1</span> Natural Language Processing</h2>
<p>Text representation can be enriched using <strong><em>N</em></strong>atural <strong><em>L</em></strong>anguage <strong><em>P</em></strong>rocessing (<strong>NPL</strong>), which provides models that go deeper to uncover the meaning. A real problem about Text processing is how phrases are recognized. There are 3 possible solutions:</p>
<ul>
<li>Use word N-grams;</li>
<li>Identify the syntactic role of words within phrases using a Part-Of-Speech tagger;</li>
<li>Store word positions in indexes and use proximity operators in queries.</li>
</ul>
<div id="part-of-speech-tagging" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Part Of Speech Tagging</h3>
<p>The <strong><em>P</em></strong>art <strong><em>O</em></strong>f <strong><em>S</em></strong>peech (<strong>POS</strong>) Tagging is a procedure that assigns to each word its syntax role in a sentence, such as nouns, verbs and adverbs. There are many applications:</p>
<ul>
<li>Parsing;</li>
<li>Word prediction in speech recognition;</li>
<li>Machine Translation;</li>
</ul>
<p>The POS Tagging problem is determine the POS tag for a particular instance of a word, because words often have more than one tag, like <code>back</code> (<em>the back door</em>, <em>on my back</em>). It is possible to assume a conditional probability of words <span class="math inline">\(\mathbb{P}(w_n | w_{n - 1})\)</span> or POS likelihood <span class="math inline">\(\mathbb{P}(t_n | t_{n - 1})\)</span> to disambiguate sentences and assessing the likelihood.</p>
<ol style="list-style-type: decimal">
<li>Start with a dictionary;</li>
<li>Assign all possible tags to word from the dictionary;</li>
<li>Write rules to selectively remove tags;</li>
<li>Leave the correct tag for each word.</li>
</ol>
</div>
<div id="named-entity-recognition" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Named Entity Recognition</h3>
<p>The <strong><em>N</em></strong>amed <strong><em>E</em></strong>ntity <strong><em>R</em></strong>ecognition (<strong>NER</strong>) is a very important task of Information Extraction because it identifies and classifies names in text in a particular application. It involves identification of proper names in text and classification into a set of predefined categories of interest.</p>
<p>Category definitions are intuitively clear, but there are many gey areas caused by metonomy, for example Organization (<code>England won the World Cup</code>) versus Location (<code>The World Cup took place in England</code>).</p>
<p>NER has different approaches:</p>
<ul>
<li><em>Ruled-Based</em>, identifying if a sequence of words can be an entity name using lexicons that categorize names (developed by Trial and Error or ML techniques);</li>
<li><em>Statistic-Based</em>, maximizing the probability of an entity using Hidden Markov Model estimated with training data and resolving ambiguity using context.</li>
</ul>
<p>HMM describes a process as a collection of states with transitions between them. Each state is associated with a probability distribution over worlds of possible outputs. For identifying named entities, it finds sequence of labels which gives the highest probability of the sentence.</p>
<!-- Lecture 6: 19/10/2020-->
<p>Natural Language is designed to make human communication efficient, but people omit a lot of common sense knowledge and keep lots of ambiguities, such as Word-Level or Syntactic ambiguity.</p>
<p>Usually, a message is sent by the sender and received by the recipient, who infers <em>Morphological</em> information of the words, uses lexical rules (<em>Syntax</em>) to reconstruct the original message and compare it with its knowledge to understand its <em>Semantic</em>, what is explicit, and does not change with the context, and <em>Pragmatic</em>, what is implicit or context related. It is the study of the relationship between language and context, fundamental to explain the understanding of the language.
In the same way, a program have to rebuild the message and compare it whit a base of knowledge (usually modeled as a graph) to understand its content (<em>Inference</em>) handling a large chunk of text (<em>Discourse</em>).</p>
<p>The Context is the situation in which the communicative act takes place and the set of knowledge, beliefs and the like are shared by both people. It includes at a minimum the beliefs and assumptions of the language users, relating to actions, situations and a state of knowledge and attention of those who partecipate to social interactions.</p>
<p>To build a computer that understands text it is necessary thus NLP Pipeline:</p>
<ol style="list-style-type: decimal">
<li>Syntactic Parsing, a grammatical analysis of a given sentence, conforming to the rules of a formal grammar;</li>
<li>Relation Extraction, identifying the relationship among named entities;</li>
<li>Logic Inference, which converts chunks of text into more formal representation.</li>
</ol>
<!-- Lecture 7: 22/10/2020 -->
</div>
</div>
</div>
<div id="language-models" class="section level1">
<h1><span class="header-section-number">4</span> Language Models</h1>
<p>A <em>Language Model</em> (<strong>LM</strong>) is a formal representation of a specific language, generating a probability distribution of the words in topics. A probability to a sequence words can be used for:
<a href="mailto:iaeQ45YB@sYwv&amp;pell" class="email">iaeQ45YB@sYwv&amp;pell</a> Correction;</p>
<ul>
<li>Speech Recognition;</li>
<li>Text Categorization;</li>
<li>Information Retrieval;</li>
<li>Summarization.</li>
</ul>
<p>The main task is to compute the probability of a sentence (or a sequence of words), <span class="math inline">\(\mathbb{P}(W) = \mathbb{P}(w_1, w_2, \dots, w_n)\)</span>, while the related task is the probability of an upcoming word: <span class="math inline">\(\mathbb{P}(w_{n + 1} | w_1, \dots, w_n)\)</span>. Any model which computes any of these procedure is called a language model.</p>
<p>The joint probability is computed relying on the chain rule of probability:</p>
<p><span class="math display">\[\begin{aligned}
    \mathbb{P}(w_1, w_2, \dots, w_n) &amp;= \mathbb{P}(w_1)\mathbb{P}(w_2 | w_1) \dots \mathbb{P}(w_n | w_1, \dots, w_{n-1}) \\
                                     &amp;= \prod_{i = 1}^n \mathbb{P}(w_i | w_1, \dots, w_{i-1})
  \end{aligned}\]</span></p>
<p>but so many product could lead the number to decay pretty fast. A solution is the Markov Assumption, an approximation of the joint probability in which considers the probability of one or two words preceding the word:</p>
<p><span class="math display">\[\mathbb{P}(w_1, w_2, \dots, w_n) \approx \prod_{i = 1}^n \mathbb{P}(w_i | w_{i - k} \dots w_{i-1}\]</span></p>
<p>In this case is possible to construct an <em>Unigram Language Model</em>, the probability distribution over the words in a language, or <em>N-gram Language Model</em>, where probabilities depend on previous words. For example a Bigram LM is the probability of a word in a sequence where considering the previous word.</p>
<p>The Maximum Likelihood Estimate of a Bigram LM is the following:</p>
<p><span class="math display">\[\mathbb{P}(w_i | w_{i-1}) = \frac{c(w_{i-1}, w_i)}{w_{i-1}}\]</span></p>
<p>For example, look at this repository:</p>
<pre><code>&lt;s&gt; I am Sam &lt;/s&gt;
&lt;s&gt; Sam I am &lt;/s&gt;
&lt;s&gt; I do not like green eggs and ham &lt;/s&gt;</code></pre>
<p>The probability of <code>I</code> at the start of the sentence is <span class="math inline">\(\mathbb{P}(I | &lt;s&gt; = 2/3 = 0.67\)</span>, while the probability of Sam in as the last word is <span class="math inline">\(\mathbb{P}(Sam | &lt;s&gt; = 1/3 = 0.33\)</span>.</p>
<p>The evaluation of a language model is the same as most important models: the dataset is splitted in training set and test set, testing the model’s performance on unseen data. The best evaluation for comparing models is compare the accuracy of those. An important metric evaluation is <em>perplexity</em>, the inverse probability of the test set normalized by number of word (minimizing perplexity is the same as maximizing probability):</p>
<p><span class="math display">\[PP(W) = \sqrt[N]{\prod_{i = 1}^N \frac{1}{\mathbb{P}(w_i | w_1 \dots w_{i-1})}}\]</span></p>
<!-- Lecture 8: 29/10/2020 -->
<div id="world-embedding" class="section level2">
<h2><span class="header-section-number">4.1</span> World Embedding</h2>
<p><em>World Embedding</em> indicates a set of techniques in a NLP where words from a vocabulary are mapped to vectors of real numbers. It involves a mathematical embedding from a space with many dimensions per word to a vector space with a lower dimension. This model can be used to check similarity between either or words and check topic similarity and words usage per topic (worlds can be represented as vectors too).</p>
<p>Each document is represented by a vector of words, with a binary representation, the frequency of the words or the weighted representation (TF-Idf).</p>
<p>Usually, the similarity is not computed by using term-document representation, because two words are similar if their context vectors are similar. To represent words as vectors, it is possible to build a <span class="math inline">\(V \times V\)</span> <em>1-hot</em> matrix (also known as <em>Local Representation</em>) in which each word has all values equal to 0 except for the corresponding colum, equal to 1. This is a very sparse matrix, with no information between similar words. To solve these problems, the idea is associate <span class="math inline">\(k\)</span> context dimensions representing properties of the words in the vocabulary, also known as <em>Distributed Representation</em>. Distributed vectors allow to group similar words together depending on the considered context.</p>
<p>The difference between Local and Distributed representation is in the matrix: in particular, in LR every term in a vocabulary <span class="math inline">\(T\)</span> is represented by a binary vector, while in DR every term is represented by a real-valued vector.</p>
<p>For simple scenarios it is possible create a <span class="math inline">\(k\)</span>-dimensional mapping for a simple example vocabulary y manually choosing contextual dimensions that make sense. This is not a simple task, in particular manual assignment of vector would be impossibly. The <em>Distributional Hypothesis</em> aims to represent each words by some context. It is possible to use different granularities of context, such as documents and sentences.</p>
<p>The <em>Window-based Co-occurence Matrix</em> counts the number of times each context words co-occurs inside a windows of a particular size with the word of interest. It generates an <span class="math inline">\(X = |V| \times |V|\)</span> co-occurence matrix. The frequency is not a good representation, it is possible to use weights, TF-IDF or <strong><em>P</em></strong>ointwise <strong><em>M</em></strong>utual <strong><em>I</em></strong>nformation (<strong>PMI</strong>):</p>
<p><span class="math display">\[PMI(w_1, w_2) = \log_2 \frac{\mathbb{P}(w_1, w_2)}{\mathbb{P}(w_1)\mathbb{P}(w_2)} \in (- \infty, + \infty)\]</span></p>
<p>To have better estimation, only positive estimation are taken:</p>
<p><span class="math display">\[PPMI(x, y) = \max [PMI(x,y),0]\]</span></p>
<p><span class="math inline">\(f_{i, j}\)</span> is the number of time the word <span class="math inline">\(w_i\)</span> appears in the context <span class="math inline">\(c_j\)</span>. It is used to compute these probabilities:</p>
<p><span class="math display">\[\begin{aligned}
    \mathbb{P}(w_j, c_j) &amp;= \frac{f_{i, j}}{\sum_{i,j = 1}^{W,C} f_{i, j}}           \\
    \mathbb{P}(w_j) &amp;= \frac{\sum_{i = 1}^C f_{i, j}}{\sum_{i,j = 1}^{W,C} f_{i, j}} \\
    \mathbb{P}(c_j) &amp;= \frac{\sum_{i = 1}^W f_{i, j}}{\sum_{i,j = 1}^{W,C} f_{i, j}} \\
  \end{aligned}\]</span></p>
<p>For example, look at this table:</p>
<pre><code>##             Computer Data Pinch Result Sugar
## Apricot            0    0     1      0     1
## Pineapple          0    0     1      0     1
## Digital            2    1     0      1     0
## Information        1    6     0      4     0</code></pre>
<p>The <span class="math inline">\(\mathbb{w_4, c_2} = 6/19 = 0.32\)</span>, <span class="math inline">\(\mathbb{w_4} = 11/19 = 0.58\)</span> and <span class="math inline">\(\mathbb{c_2} = 7/19 = 0.37\)</span>. The PPMI table is:</p>
<pre><code>##             Computer Data Pinch Result Sugar
## Apricot         0.00 0.00  0.05  0.000  0.05
## Pineapple       0.00 0.00  0.05  0.000  0.05
## Digital         0.11 0.05  0.05  0.050  0.00
## Information     0.05 0.32  0.00  0.021  0.05</code></pre>
<p>Rare words and apax produce higher scores, that risks to bias the analysis: probabilities of <span class="math inline">\(w\)</span> and <span class="math inline">\(c\)</span> are modified or a <span class="math inline">\(k\)</span>-smoothing is used to reduced those scores with a value <span class="math inline">\(\alpha \in [0, 1]\)</span> (usually <span class="math inline">\(\alpha = 0.75\)</span>):</p>
<p><span class="math display">\[PMI_{\alpha}(w, c) = \log_2 \frac{\mathbb{P}(w, c)}{\mathbb{P}(w)\mathbb{P}_{\alpha}(c)} \in (- \infty, + \infty)
\implies
PPMI_{\alpha}(w, c) = \max [(PMI_{\alpha}(w, c), 0]\]</span></p>
<p>with</p>
<p><span class="math display">\[P_{\alpha}(c) = \frac{\text{count}(c)^{\alpha}}{\sum_c \text{count}^{\alpha}}\]</span></p>
<!-- Lecture 10: 02/11/2020 -->
<p>A Co-occurrence Matrix in reality is constituted by a very large number of words. For each word, Tf-Idf and PPMI vectors are long (<span class="math inline">\(|V| \approx 50&#39;000\)</span>) and sparse (most elements <span class="math inline">\(= 0\)</span>). There are techniques to learn lower-dimensional vectors for words, which are Short and Sense. These dense vectors in a latent space are called <em>Embedding</em>.</p>
<!-- Two type of learning embedding models -->
<div id="count-based-models" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Count-Based Models</h3>
<p><em>Count-Based Models</em> computes the statistic of how often some words co-occurs with its neighbor words in a large text corpus. Then it is possible to reduce the dimensionality the vector in an small and dense array for each word. The lower-dimensional matrix of words and features represent a dense vector for each word.</p>
<p>The most popular model is the <strong><em>S</em></strong>ingular <strong><em>V</em></strong>alue <strong><em>D</em></strong>ecomposition (<strong>SVD</strong>), a dimensionality reduction techniques used in Text Mining. Any rectangular <span class="math inline">\(w \times c\)</span> matrix <span class="math inline">\(X\)</span> can be expressed as the product of 3 matrices:</p>
<p><span class="math display">\[\underset{w \times c}{X} = \underset{w \times m}{U}
                             \times \underset{m \times m}{S} 
                             \times \underset{m \times c}{V^t}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(U\)</span> is an <span class="math inline">\(w \times m\)</span> matrix with <span class="math inline">\(w\)</span> words of the original <span class="math inline">\(X\)</span> matrix with <span class="math inline">\(m\)</span> features in a new latent space;</li>
<li><span class="math inline">\(S\)</span> is an <span class="math inline">\(m \times m\)</span> diagonal matrix of <em>Singular Values</em> expressing the importance of each dimension;</li>
<li><span class="math inline">\(V^t\)</span> is an <span class="math inline">\(m \times c\)</span> matrix with <span class="math inline">\(c\)</span> features of the original matrix <span class="math inline">\(X\)</span> and <span class="math inline">\(m\)</span> singular values rows.S</li>
</ul>
<p>It is possible to select the top-<span class="math inline">\(k\)</span> singular values obtaining a low rank approximation of the original matrix <span class="math inline">\(X\)</span>. Instead of multiplying these matrices, it is useful to make only use of the matrix <span class="math inline">\(U\)</span>. Each row of <span class="math inline">\(U\)</span> contains a <span class="math inline">\(k\)</span>-dimensional vector representing a word in the vocabulary.</p>
<!-- Singular Value Decomposition Image -->
<p><img src="Immagini/SVD.png" width="100%" style="display: block; margin: auto;" /></p>
<p><span class="math display">\[\underset{w \times c}{X} \approx  \underset{w \times k}{U}
                                    \times \underset{m \times k}{S} 
                                    \times \underset{k \times c}{V^t}\]</span></p>
<!-- Italic words = main point  -->
<p>Unfortunately, there are many drawbacks regarding this method. First of all, the <em>Dimension</em> of the matrix <em>Change</em> very often: new words are added very frequently and corpus changes in size. Secondly, the matrix is extremely <em>Sparse</em> since most word do not occur and it is necessary to construct a quadratic loss function to perform an SVD. To resolve these problems there are some hacks on <span class="math inline">\(X\)</span> obtaining more satisfying values: ignoring <em>Stop Words</em>, applying a <em>Ramp Window</em>, a co-occurrence count based on the distance between the words in the document or use the <em>Pearson Correlation</em> and set negative counts to 0 instead of using PPMI.</p>
<p>Another important Count-Based model is the <strong><em>Glo</em></strong>bal <strong><em>Ve</em></strong>ctors for Word Representations (<strong>GloVe</strong>), a model that leverages statistical information by training only non-zero element in a word-word co-occurrence matrix, rather than on the entire sparse matrix (SVD) and on individual context windows in a large corpus (Word2Vec). In particular, global corpus statistics are captured directly by the model.</p>
<!--
$X$ is the term-context matrix
$X_{ij}$ is the frequency of the word $j$ occuring in the context of word $i$
$X_i = \sum_k X_{ik}$ is the global frequency of any word appearing in the context of word $i$
$P_{ij} = P(j|i) = \frac{X_{ij}}{X_i}$ is the probability that word $j$ appears in the context of word $i$ -> co-occurrence probability
-->
<!--rivedere questa riga, 11:23 lunedì-->
<p>In the glove model, instead of considering just the p of the context word appearing, we are going considering the ration of the probability of the words. Only in the relation capture non discriminative words because large values (&gt; 1) correlate well with properties specific to a word and</p>
<p>The ration <span class="math inline">\(P_{ik} / P_{Jk}\)</span> depends on three words <span class="math inline">\(i,j\)</span> and <span class="math inline">\(k\)</span>. The general form of the glove model is useful to learn the word vectors representation:</p>
<p><span class="math display">\[\begin{aligned}
    F(w_i, w_j, \tilde{w_k}) = \frac{P_ik}{P_jk} \\
    w_i^t \tilde{w_k} + b_i + \tilde{b_k} = \log(X_{ik})
  \end{aligned}\]</span></p>
<p>where <span class="math inline">\(w \in R^d\)</span> are word vectors and <span class="math inline">\(\tilde{w_k} \in R^d\)</span> are separate context word vectors.</p>
<p>The GloVe model builds an objective function <span class="math inline">\(J\)</span> associating word vectors to text statistics using the Least Square Error with a weighted function <span class="math inline">\(f(X_{ik})\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
    J &amp;= \sum_{i,k = 1}^V f(X_{ik})(w_i^t w_k + b_i + b_k - \log(X_{ik}) \\
      &amp;= \frac{1}{2} \sum_{i,k = 1}^V f(X_{ik}) (w_i^t \tilde{w_k} - \log(X_{ik}))^2
  \end{aligned}\]</span></p>
<p>where <span class="math inline">\(V\)</span> is the size of the vocabulary.</p>
<p>The two sets of vectors captures similar co-occurence information. The best solution is to simply sum them up:</p>
<p><span class="math display">\[X_{final} = U + V\]</span></p>
</div>
</div>
<div id="predictive-models" class="section level2">
<h2><span class="header-section-number">4.2</span> Predictive Models</h2>
<p><em>Predictive Models</em> predict a word from its neighbors obtaining a small, dense embedding vectors. The most popular models is the <em>Word2Vec</em> model, which creates collections of similar concept automatically on raw texts (supervised training data) and without advanced language skills. Very good performance are obtained by employing very large (<span class="math inline">\(\approx 10\)</span>M words) text including as many different words as possible.</p>
<p>It is similar to Language Modelling, but it predicts the context rather than next word. The loss function is the following:</p>
<p><span class="math display">\[J = 1 - P(\text{context } | \text{ word})\]</span></p>
<p>There are two training method:</p>
<ul>
<li>Hierarchical Softmax;</li>
<li>Negative Sampling.</li>
</ul>
<!-- Two type of architecture models -->
<p>The <em>Skip Gram</em> model predicts the surrounding (context) words based on the current (centered) word. Given a sliding window of a fixed size moving along a sentence, the word in the middle is the target, while near words (in the sliding window) are context words. Then it is trained to predict the probabilities of a word being a context word for the given target. The model is a Neural Network with just one Hidden Layer representing the word embedding of size <span class="math inline">\(N\)</span>. The Input layer <span class="math inline">\(x\)</span> and the Output Layer <span class="math inline">\(y\)</span> are one-hot encoded word representations.</p>
<!-- Skip Gram Model -->
<p><img src="Immagini/Skip-Gram.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The <strong><em>C</em></strong>ontinuos <strong>B</strong>ag <strong><em>O</em></strong>f <strong><em>W</em></strong>ords (<strong>CBOW</strong>) is another similar model learning for learning word vectors. It predicts the target word from source context words.</p>
<!-- CBOW Model -->
<p><img src="Immagini/CBOW.png" width="100%" style="display: block; margin: auto;" /></p>
<!-- Differences -->
<p>While the Skip Gram model works well with a smaller training dataset because it represents well even rare words, the CBOW is several times faster to training the model and has a slight better Accuracy for frequent words.</p>
<!-- Pros and Cons of Predictive Models-->
<p>Predictive models can capture complex pattern beyond word similarity and generates improved performance on other tasks, but it scales with corpus size (it is necessary to train every time the data) and it does not use efficient statistics.</p>
<!-- Lecture 11: 05/11/2020 -->
</div>
</div>
<div id="text-classification" class="section level1">
<h1><span class="header-section-number">5</span> Text Classification</h1>
<p><em>Text Classification</em> is the activity of predicting to which data items belongs to a predefined finite set of classes. In text classification, data items can be textual (news articles, e-mail, etc.) or partly textual (web pages).</p>
<p><span class="math display">\[h : D \rightarrow C\]</span></p>
<p>There are many types of classification:</p>
<ul>
<li><em>Binary Classification</em>, where each items belongs to exactly one classes in a set of two;</li>
<li><em>Single Label Multi Class Classification</em>, where each item belongs to exactly one classes in a set of many classes (&gt; 2);</li>
<li><em>Multi Label Multi Class Classification</em>, where each item may belong to zero, one or several classes together;</li>
<li><em>Ordinal Classification</em>, similar to <em>SLMC</em>, but classification are ordered respectably to some quality.</li>
</ul>
<p>The previous definitions denote hard classification (training soft classifier with a score and picking a threshold <span class="math inline">\(t\)</span>). Soft classification aims to predict a score for each pair <span class="math inline">\((d, c)\)</span> where the score denotes the probability that d belong to c, where scores are used for ranking.</p>
<div id="applications" class="section level2">
<h2><span class="header-section-number">5.1</span> Applications</h2>
<p>There are many applications:</p>
<ul>
<li><em>Knowledge Organization</em>, conferring structure to an otherwise unstructured body of knowledge (e.g. classifying news articles);</li>
<li><em>Filtering</em>, blocking a set of non relevant items from a dynamic stream. It is a binary classification (e.g. spam filtering detecting unsuitable content);</li>
<li><em>Empowering</em>, improving the effectiveness of other tasks in Information Retrieval or Natural Language Process.</li>
</ul>
</div>
<div id="supervised-learning" class="section level2">
<h2><span class="header-section-number">5.2</span> Supervised Learning</h2>
<p>The Supervised Learning is a generic learning algorithm used to train a classifier from a set of manually classified examples. The classifiers learns the characteristics that a new text should have in order to be assigned to class <span class="math inline">\(c\)</span>.</p>
<p>In order to generate a vector based representation for a set of documents, there are 3 main steps:</p>
<ol style="list-style-type: decimal">
<li><em>Feature Extraction</em> by topic, a set of features that coincide with the set of words occurring in the training set (Unigram model). An alternative is make the set of features coincide with the set of character <span class="math inline">\(n\)</span>-grams occurring in the document. The choice of features is different for classification tasks;</li>
<li><em>Feature Selection</em>, identifying the most discriminative features discarding the others. The filter approach consists in measuring the discriminative power of each feature and retaining only the top-scoring features. A typical choice is Mutual Information, measuring the mutual dependence between two variables. It can be used also with matrix decomposition like SVD, PCA, aggregating different features:</li>
</ol>
<p><span class="math display">\[MI(t_k, c_i) = \sum \sum \mathbb{P}(t, c) \log_2 \frac{\mathbb{P}(t, c)}{\mathbb{P}(t) \mathbb{P}(c)}\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Feature weighting means attributing a value to feature in the vector that represents document. This value can be binary, numeric obtained in a unsupervised or supervised classes.</li>
</ol>
<!-- Lecture 13: 09/11/2020-->
<p>The classical way to represent documents is the vector space model, where documents are converted into vectors in a common vector space. It is constituted by terms (axes) and documents are points in the vector space. To represent the similarity of a document is possible to use the cosine similarity:</p>
<p><span class="math display">\[\mathbb{sim}(d_1, d_2) = \frac{\sum_{i=1}^k w_{i1}w_{i2}}{\sqrt{\sum_{i=1}^k w_{i1}^2 \sum_{i=1}^k w_{i2}}^2} \in [0, 1]\]</span></p>
</div>
<div id="supervised-classification" class="section level2">
<h2><span class="header-section-number">5.3</span> Supervised Classification</h2>
<p>For binary classification, essentially any supervised learning algorithm can be used for training a classifier. One of the most important method is <strong><em>S</em></strong>upport <strong><em>V</em></strong>ector <strong><em>M</em></strong>achine (<strong>SVM</strong>), which aims to find the separating surface that maximizes the margin between hyperplane and training points. Often classification problems are not linearly separable, but it can become linear separable once mapped to an high-dimensional space using kernels. The trained classifiers often depend on one or more parameters, but they need to be optimized with a <span class="math inline">\(k\)</span>-fold cross-validation on the training set.</p>
</div>
<div id="evaluations" class="section level2">
<h2><span class="header-section-number">5.4</span> Evaluations</h2>
<p>There two important aspects to evaluate a classifier: <em>Efficiency</em>, which refers to the consumption of computational resources (training and classification), while Effectiveness (<em>Accuracy</em>) refers to how frequently classification decisions taken by a classifier are correct. For binary classification it is possible to evaluate accuracy with confusion matrix:</p>
<p><span class="math display">\[Accuracy = \frac{TN + TP}{TN + FN + FP + TP}\]</span></p>
<p>Another measure is <span class="math inline">\(F_1\)</span>, robust to the presence of imbalance in the test set. It is defined as the harmonic mean of Precision and Recall:</p>
<p><span class="math display">\[F_1 = \frac{\pi \rho}{\pi + \rho} = \frac{2 TP}{2 TP + FP + FN}\]</span></p>
<p>For multi-label multi-class classification, <span class="math inline">\(F_1\)</span> must be averaged across the classes according to micro-averaging, or macro-averaging.</p>
<p>For single-label multi-class classification, the most used measure is the Accuracy.</p>
<!-- Lecture 14: 12/11/2020 -->
</div>
</div>
<div id="text-clustering" class="section level1">
<h1><span class="header-section-number">6</span> Text Clustering</h1>
<p>The Document Clustering is the process of grouping a set of documents into classes of similar object. It is an unsupervised learning, so clusters are formed from data without human input. In particular:</p>
<ul>
<li>Object <em>within</em> a cluster should be similar;</li>
<li>Object <em>from different</em> clusters should be dissimilar.</li>
</ul>
<p>It is possible to use Cluster Analysis with documents because objects in the same cluster behave similarly with respect to relevance to information needs. There are different types of clustering structure:</p>
<ul>
<li><em>Partitional</em>, a division of the set of data objects into non-overlapping subsets (clusters) such that each data object is in exactly one subset vs <em>Hierarchical</em>, sub-clusters organized as a tree.</li>
<li><em>Exclusive</em>, each data is assigned to a single cluster vs <em>Overlapping</em>, data can be assigned in more than one cluster.</li>
<li><em>Complete</em>, every data is assigned to a cluster vs <em>Partial</em>, where every data is not necessary assigned to a cluster.</li>
</ul>
<div id="flat-clustering" class="section level2">
<h2><span class="header-section-number">6.1</span> Flat Clustering</h2>
<p><em>Flat Clustering</em>, known also as Prototype-Based Clustering, is a clustering algorithm in which any object is closer to the prototype that defines the cluster to which it belongs to than to the prototype of any other cluster.</p>
<p>The goal of this algorithm is compute an assignment <span class="math inline">\(\gamma : D \rightarrow \{w_1, \dots, w_k\}\)</span> that minimize the objective function (in term of similarity or distance) given a set of documents <span class="math inline">\(D\)</span>, a desired number of clusters <span class="math inline">\(k\)</span>. <em>k-means</em> defines a prototype in terms of a centroid, which is usually the mean (average squared Euclidean Distance) of the documents in the real-valued space. The <span class="math inline">\(k\)</span>-means algorithm is defined as follows:</p>
<ol style="list-style-type: decimal">
<li>Select <span class="math inline">\(k\)</span> objects as centroids;</li>
<li>Form <span class="math inline">\(k\)</span> clusters and assign each document to closest centroid;</li>
<li>Compute the centroid for each cluster;</li>
<li>Go to <strong>1.</strong> until centroid do not change.</li>
</ol>
<!-- Lecture 15: 16/11/2020 -->
</div>
<div id="hierarchical-clustering" class="section level2">
<h2><span class="header-section-number">6.2</span> Hierarchical Clustering</h2>
<p><em>Hierarchical Clustering</em> is a clustering algorithm in which sub-clusters are organized as a tree. It also does not require to specify the number of clusters. It can be:</p>
<ul>
<li><em>Agglomerative</em>, where all single objects are merged into the closest pair of cluster (by proximity);</li>
<li><em>Divisive</em>, where an all-inclusive cluster is splitted in different clusters until only single clusters of individual object remain using a flat clustering algorithm (it is necessary to specify which cluster to split and the cut of the dendrogram).</li>
</ul>
<p>Hierarchical Clustering is often displayed graphically using a tree-like diagram called <em>Dendrogram</em>, which displays the sub-cluster relationships and the order in which the clusters were merged or splitted.</p>
<!-- Dendrogram Image -->
<p><img src="Immagini/Dendrogram.png" width="100%" style="display: block; margin: auto;" /></p>
<p>There are different ways to define the closest pair of cluster:</p>
<ul>
<li><em>Single-link</em>, where the similarity of 2 clusters is the similarity of their most similar members (find the minimum distance between two clusters):</li>
</ul>
<p><span class="math display">\[\begin{aligned}
    d(w_i, w_j)            &amp;= min_{x \in w_i, y \in w_j} d(x, y) \\
    d((w_i \cup w_j), w_k) &amp;= min [d(w_i, w_k), d(w_j, w_k)]
  \end{aligned}\]</span></p>
<ul>
<li><em>Complete-link</em>, where the similarity of 2 clusters is the similarity of their most dissimilar members (find the maximum distance between the elements):</li>
</ul>
<p><span class="math display">\[\begin{aligned}
    S(w_i, w_j)            &amp;= min_{x \in w_i, y \in w_j} S(x, y) \\
    S((w_i \cup w_j), w_k) &amp;= min [S(w_i, w_k), S(w_j, w_k)]
  \end{aligned}\]</span></p>
<ul>
<li>Group-average;</li>
<li>Centroid.</li>
</ul>
</div>
<div id="clustering-evaluation" class="section level2">
<h2><span class="header-section-number">6.3</span> Clustering Evaluation</h2>
<p>It is difficult to evaluate the validation of a cluster algorithm. A good clustering will produce high quality clusters in which the external class similarity is high and the internal class similarity is low.</p>
<p>The most important <em>Internal Evaluation</em> index is the Silhouette Coefficient, that measures how well an observation is clustered and estimates the average distance between clusters. It is defined as follows:</p>
<p><span class="math display">\[S_i = \frac{b_i - a_i}{\max(a_i, b_i)} \in [-1, +1]\]</span></p>
<ul>
<li><span class="math inline">\(a_i\)</span> is the average distance of the <span class="math inline">\(i\)</span>-th object to all other objects in its cluster;</li>
<li><span class="math inline">\(b_i\)</span> is the minimum of the average distances of the <span class="math inline">\(i\)</span>-th object to all the objects in each given cluster.</li>
</ul>
<p>The most important <em>External Measure</em> is the Purity, defined as the ratio between the dominant class and the size of the cluster:</p>
<p><span class="math display">\[Purity(w_i) = \frac{1}{n_i} \max_j (n_{ij}) \in [0, 1]\]</span></p>
<p>Other external measures are:</p>
<ul>
<li>Rand Index, defined as the percentage of correct assigment to a cluster: <span class="math inline">\(RI = \frac{TP + TN}{N} \in [0, 1]\)</span>;</li>
<li>Precision, Recall and <span class="math inline">\(F\)</span>-measure.</li>
</ul>
<!-- Lecture 16: 19/11/2020 -->
</div>
</div>
<div id="topic-modeling" class="section level1">
<h1><span class="header-section-number">7</span> Topic Modeling</h1>
<p>Topic Modeling is an unsupervised machine learning tecnique aimed at scanning a set of documents, detect words and phrases patterns within them and automatically clustering word groups which best characterize a set of documents. It provides a set of words that makes sense together (<em>topic</em>).</p>
<!-- Text Clustering Vs Topic Modelling -->
<p>The main differences between Text Clustering and Topic Modelling is that the first procedure groups documents into different clusters based on similarity (or distance) measures, while the second one groups words into different clusters where each word have a probability of occurrence for the given topic.</p>
<!-- Techniques:
     
     * Latent Semantic Analysis (LSA)
     * Latent Dirichlet Analysis (LDA) -->
<div id="latent-semantic-analysis" class="section level2">
<h2><span class="header-section-number">7.1</span> Latent Semantic Analysis</h2>
<p>The <strong><em>L</em></strong>atent <strong><em>S</em></strong>emantic <strong><em>A</em></strong>nalysis (<strong>LSA</strong>) is a Topic Modeling Technique which computes how frequently words occur in the documents and assumes that similar documents will contain the same distribution of word frequencies. The main idea is decompose the Document Term Matrix into a Document Topic and Topic Term matrix.</p>
<p>The standard method for computing word frequencies is the Tf-Idf for each term in a document. It can be decomposed using SVD:</p>
<p><span class="math display">\[X = \underset{\text{Document Topic}}{U} \times
      \underset{\text{Potential Topic}}{S} \times
      \underset{\text{Term Topic}}{V^t}\]</span></p>
<!-- Cons -->
<p>The problem of dimensionality reduction is that cannot have an interpretable meaning in the natural language. Furthermore LSA can onlu capture partially polysemy, but this is not a problem because it can find the predominant sense in a document.</p>
<!-- Alternative -->
<p>An alternative of this techniques is the <em>Probabilistic LSA</em> (<strong>pLSA</strong>), which identifies and distinguishes words in different contexts whithout using the dictionary. In particular, it allows to disambiguate polysemy and discloses topical similarities grouping together words sharing the same context.</p>
<p>It finds a probabilistic model with hidden (or latent) topic that can generate the observed data in the Document Term Matrix. pLSA expresses data in three different variables:</p>
<ul>
<li><em>Documents</em> observed variables: <span class="math inline">\(d = \{d_1, \dots, d_N \} \in D\)</span>;</li>
<li><em>Words</em> observed variables: <span class="math inline">\(w = \{w_1, \dots, w_M \} \in W\)</span>;</li>
<li><em>Topics</em> hidden variables: <span class="math inline">\(z = \{z_1, \dots, z_K \} \in Z\)</span>.</li>
</ul>
<p><span class="math display">\[\begin{aligned}
    \mathbb{P}(D, W) &amp;= \prod_{(d, w)} \mathbb{P}(d, w) \\
                     &amp;= \sum_{Z \in z} \mathbb{P}(z)\mathbb{P}(d|z)\mathbb{P}(w|z) \\
    \implies A       &amp;\approx U_kS_kV_k^t
  \end{aligned}\]</span></p>
<!-- Lecture 17: 23/11/2020 -->
</div>
<div id="latent-dirichlet-analysis" class="section level2">
<h2><span class="header-section-number">7.2</span> Latent Dirichlet Analysis</h2>
<p>The <strong><em>L</em></strong>atent <strong><em>D</em></strong>irichlet <strong><em>A</em></strong>nalysis (<strong>LDA</strong>) is the Bayesian version of pLSA. It categorizes documents, treated as bag of Words, by topic via a generative probabilistic model assuming they are produced from a mixture of topics. Dirichlets distributions is better in finding the assignment of documents to a general topic.</p>
<p>In a corpus of <span class="math inline">\(M\)</span> documents, it is necessary to choose <span class="math inline">\(k\)</span> topics to discover. LDA outputs the topic model and the <span class="math inline">\(M\)</span> documents expressed a combination of topics finding the weight of connections between documents and topics and words. The algorithm creates an intermediate layer with topics and finds the weights. In this case documents are no longer connected to words but to topics.</p>
<p>A Dirchlet distribution <span class="math inline">\(Dir(p)\)</span> is a probability funtion which gives probabilities for discrete random variables. It includes the concentration parameter <span class="math inline">\(p\)</span> that rules the trend of the distribution:</p>
<ul>
<li>Sparse, producing a real life distribution: <span class="math inline">\(p &lt; 1\)</span>;</li>
<li>Uniform, producing a random distribution: <span class="math inline">\(p = 1\)</span>;</li>
<li>Concetrated: <span class="math inline">\(p &gt; 1\)</span>.</li>
</ul>
<!-- Dirchlet Distribution Image -->
<p><img src="Immagini/Dirchlet-Distribution.png" width="100%" style="display: block; margin: auto;" /></p>
<p>In LM there are two probability functions:</p>
<ul>
<li><span class="math inline">\(\theta_d \approx Dir(\alpha)\)</span>, the probability of topic <span class="math inline">\(k\)</span> occurring in document <span class="math inline">\(d\)</span>;</li>
<li><span class="math inline">\(psi_k \approx Dir(\beta)\)</span>, the probability of word <span class="math inline">\(w\)</span> occurring in topic <span class="math inline">\(k\)</span>.</li>
</ul>
<!-- LSA vs LDA -->
<p>The main differnce between LSA and LSA is the assumption of the Dirchlet distribution, while the second does not assume any distribution. LSA works better than pLSA because it can generalize to new documents.</p>
</div>
<div id="evaluation" class="section level2">
<h2><span class="header-section-number">7.3</span> Evaluation</h2>
<p>There are different approaches evaluating topic modeling:</p>
<ul>
<li>Eye Balling Models, identifying the top <span class="math inline">\(n\)</span> words in a documents;</li>
<li>Intrinsic Evaluation Metrics, using the <em>Perplexity</em>, a measure of uncertainty, and the <em>Coherence</em>, measuring the semantic similarity between top words whitin the topic. They help to distinguish topics in an interpretable topics from topics that are artifacts of statistical inference;</li>
<li>Human Judgments, so what is a topic;</li>
<li>Extrinsic Evaluation Metrics, so if the model is good at performing task, such as classification.</li>
</ul>
</div>
</div>
<div id="topic-classification" class="section level1">
<h1><span class="header-section-number">8</span> Topic Classification</h1>
<p>Topic Classification is a supervised Machine Learning technique in which there are a list of predefined topic for a set of texts. There are many approaches:</p>
<ul>
<li>Rule-based Systems, programming a set of hand made rules based on the content of the documents</li>
<li>Machine Learning Systems, like ML;</li>
<li>Hybrid Systems, a mixture of the previuous two.</li>
</ul>
</div>
<div id="text-summarization" class="section level1">
<h1><span class="header-section-number">9</span> Text Summarization</h1>
<p><em>Text Summariztion</em> is the process of finding the most important information from a source to produce an abridged version of a text for a user. Main advantages are the reduced reading time and the easy research of a document. Automatic summarization improves the effectiveness of indexing. There are different dimension of the Text Summarization:</p>
<ul>
<li><em>Based on Input Type</em>, the process of summarizing the text, given a single document (or a group of documents);</li>
<li><em>Based on the Purposes</em>, that can be:
<ul>
<li>Generic, no assumptions about the content of the text to be summarized (it is a generic summarization);</li>
<li>Domain-Specific, using a domain knowledge forming a more accurate summary;</li>
<li>Query-Based, summarizes a document to an information need expressed in a query. Sometimes there is also a snippet summarizing a Web page.</li>
</ul></li>
<li><em>Based on Output Type</em>, that can be:
<ul>
<li>Extractive Summarization, which selects some important phrases from the input text. This summarization has 3 different tasks:
<ol style="list-style-type: decimal">
<li>Create an intermediate representation of the input text capturing the key aspects. There is Topic Representation, an intermediate representation capturing topic in the input text and sentences are scored for the importance <!-- considera i punteggi in base all'importanza -->. It is possible to use topic words (table of words and corresponding weights), LSA or LDA. There is also Indicator Representation, where the text is represented by different set of possible indicators of importance. <!-- considera i punteggi in base a degli indicatori --> It is possible to represent each sentence as a list of indicators of importance (E.g. sentence length and location in the document).</li>
<li>Scoring sentences based on the representation, where each sentence is assigned to a scoe indicating its importance. The weights of each sentence is determined combining different indicators using Machine Learning techniques;</li>
<li>Selecting a summary consisting of several scored sentences. It is possible to show the best <span class="math inline">\(n\)</span> approaches or the maximal marginale relevance approaches;</li>
</ol></li>
<li>Abstractive Summarization, which captures the meaning of the text and producing a summary based on its own prhases (more appealing).</li>
</ul></li>
</ul>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
